{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Your First Submission - Get on the Leaderboard in 15 Minutes!\n",
    "\n",
    "Let's create a first submission and get on the leaderboard.\n",
    "\n",
    "## Today's mission\n",
    "\n",
    "- Submit SOMETHING that works (even if it's terrible)\n",
    "- Learn why notebooks suck for production (and how to fix it)\n",
    "- Write actual tests (yes, even for a hackathon)\n",
    "- Understand the scoring by failing fast\n",
    "\n",
    "Your first submission will score terribly. Ship it anyway. You learn more from one bad submission than from endless planning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Project Structure (The Right Way‚Ñ¢)\n",
    "\n",
    "Remember Tutorial 2 where we just dumped everything in the notebook? Yeah, that doesn't scale. Let's be slightly more professional.\n",
    "\n",
    "**Our structure:**\n",
    "```\n",
    "GDSC-8/\n",
    "‚îú‚îÄ‚îÄ src/           # Reusable code goes here\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils.py   # We've created this for you!\n",
    "‚îú‚îÄ‚îÄ tests/         # Yes, we're writing tests\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n",
    "‚îî‚îÄ‚îÄ tutorials/     # Notebooks for exploration\n",
    "```\n",
    "\n",
    "**üìù Note**: We've created `src/utils.py` for you with submission and validation functions. This keeps the tutorial focused on the matching logic, but definitely peek at the source code to understand what's happening under the hood!\n",
    "\n",
    "**Why this structure matters:**\n",
    "- Notebooks = great for exploration, terrible for production\n",
    "- Modules = testable, reusable, version-controllable\n",
    "- Tests = confidence that your code actually works\n",
    "\n",
    "Let's import our utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported from src/ like a pro\n",
      "üí° Production tip: Keep reusable code in modules, not notebooks\n"
     ]
    }
   ],
   "source": [
    "# Setup imports - the professional way\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path so we can import from src/\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our utilities\n",
    "from src.utils import (\n",
    "    make_submission,\n",
    "    save_json,\n",
    "    validate_submission_format,\n",
    "    sanity_check\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Imported from src/ like a pro\")\n",
    "print(\"üí° Production tip: Keep reusable code in modules, not notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Sanity Check\n",
    "\n",
    "Before we submit anything, let's make sure our AWS connection works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå API check failed with status: 403\n",
      "\n",
      "‚ö†Ô∏è Fix your AWS credentials first!\n",
      "Check Tutorial 1 for setup instructions\n",
      "üí° Tip: Use Copilot (Ctrl+I) to help debug connection issues\n"
     ]
    }
   ],
   "source": [
    "# Check API connection\n",
    "# TODO for testers: If this fails, check AWS credentials and network access\n",
    "\n",
    "if sanity_check():\n",
    "    print(\"\\nüéØ Ready to submit!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è You are not connected to the API\")\n",
    "    print(\"\\nIf you are using Sagemaker Notebook the AWS credentials should be set by default and the sanity check should work. If not, contact the organization team.\")\n",
    "    print(\"\\nIf you are running this locally, ensure your AWS credentials are configured correctly. Run aws sts get-caller-identity to verify.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The World's Laziest Matcher\n",
    "\n",
    "Let's build the absolute minimum viable submission. Everyone gets the same job. Zero intelligence. Zero API calls. Maximum speed.\n",
    "\n",
    "**Why start here?**\n",
    "- Understand the submission format\n",
    "- Test the full pipeline\n",
    "- Get that psychological win of being on the leaderboard\n",
    "- Establish a baseline (spoiler: it'll be bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Scoring System (This is Critical!)\n",
    "\n",
    "Before we build anything, let's understand how your submission is actually scored. This knowledge alone can boost your score by 10-20%.\n",
    "\n",
    "### The Two-Part Scoring Formula\n",
    "\n",
    "Your final score = **50% Type Accuracy + 50% Recommendation Accuracy**\n",
    "\n",
    "**Part 1: Type Accuracy (50% of your score)**\n",
    "- Did you correctly predict if the persona needs `jobs+trainings`, `trainings_only`, or `awareness`?\n",
    "- This is binary - you either get it right (1.0) or wrong (0.0)\n",
    "- **Hidden gotcha**: Minors (age < 16) should ALWAYS get `awareness` type with `predicted_items: \"too_young\"`\n",
    "\n",
    "**Part 2: Recommendation Accuracy (50% of your score)**\n",
    "How this is calculated depends on the type:\n",
    "\n",
    "For `jobs+trainings`:\n",
    "- 25% of total score: F1 score on job matches\n",
    "- 25% of total score: F1 score on training suggestions per job\n",
    "- Yes, training suggestions matter that much!\n",
    "\n",
    "For `trainings_only`:\n",
    "- 50% of total score: F1 score on training recommendations\n",
    "\n",
    "For `awareness`:\n",
    "- 50% of total score: Exact match on reason (e.g., \"too_young\")\n",
    "\n",
    "### The Critical Insight\n",
    "\n",
    "**If you get the type wrong, your recommendation score is ZERO!**\n",
    "\n",
    "Example: You recommend perfect jobs for someone, but they actually needed `trainings_only`. Your score for that persona: 0%.\n",
    "\n",
    "This is why understanding personas is crucial. Getting the type right is literally half your score.\n",
    "\n",
    "### Quick Win Strategy\n",
    "\n",
    "1. **Check ages first** - Anyone under 16 ‚Üí `awareness` with `predicted_items: \"too_young\"`\n",
    "2. **Default to `jobs+trainings`** - Most personas want jobs\n",
    "3. **Always include training suggestions** - They're 25% of your score for job recommendations!\n",
    "\n",
    "Now let's build our submissions with this knowledge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Lazy Matcher v1 Stats:\n",
      "  Predictions: 100\n",
      "  Unique jobs recommended: 1\n",
      "  API calls made: 0\n",
      "  Cost: $0.00\n",
      "  Expected score: Terrible\n",
      "  Time to implement: 30 seconds\n"
     ]
    }
   ],
   "source": [
    "def lazy_matcher_v1():\n",
    "    \"\"\"\n",
    "    The laziest possible solution that still works.\n",
    "    Everyone gets job_001. No personalization. No intelligence.\n",
    "\n",
    "    This is your baseline - everything else should beat this.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(1, 101):\n",
    "        results.append({\n",
    "            \"persona_id\": f\"persona_{i:03}\",\n",
    "            \"predicted_type\": \"jobs+trainings\",\n",
    "            \"jobs\": [\n",
    "                {\n",
    "                    \"job_id\": \"job_001\",\n",
    "                    \"suggested_trainings\": []  # No training suggestions\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Generate our terrible predictions\n",
    "results_v1 = lazy_matcher_v1()\n",
    "\n",
    "print(\"üìä Lazy Matcher v1 Stats:\")\n",
    "print(f\"  Predictions: {len(results_v1)}\")\n",
    "print(f\"  Unique jobs recommended: 1\")\n",
    "print(f\"  API calls made: 0\")\n",
    "print(f\"  Cost: $0.00\")\n",
    "print(f\"  Expected score: Terrible\")\n",
    "print(f\"  Time to implement: 30 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Validate Before Submitting\n",
    "\n",
    "**Pro tip**: Always validate your format before submitting. Catching errors locally is free. Debugging failed submissions is painful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validated 100 results - format is correct!\n",
      "‚úÖ Format is valid! Ready to submit\n"
     ]
    }
   ],
   "source": [
    "# Always validate before submitting!\n",
    "try:\n",
    "    validate_submission_format(results_v1)\n",
    "    print(\"‚úÖ Format is valid! Ready to submit\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Format error: {e}\")\n",
    "    print(\"Fix this before submitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Your Work\n",
    "\n",
    "**Best practice**: Always save your submissions. You'll want to compare different approaches later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Submission Format\n",
    "\n",
    "**What's JSONL?** It's just JSON objects, one per line. Perfect for streaming large datasets without loading everything into memory.\n",
    "\n",
    "```jsonl\n",
    "{\"persona_id\": \"persona_001\", \"predicted_type\": \"jobs+trainings\", \"jobs\": [...]}\n",
    "{\"persona_id\": \"persona_002\", \"predicted_type\": \"jobs+trainings\", \"jobs\": [...]}\n",
    "```\n",
    "\n",
    "[Learn more about JSONL format](https://jsonlines.org/) or ask Copilot (Ctrl+I) to explain the difference between JSON and JSONL.\n",
    "\n",
    "Our `save_json()` function handles the conversion automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved to submissions/lazy_v1.json\n",
      "\n",
      "üí° Tip: Keep all your submissions for comparison\n"
     ]
    }
   ],
   "source": [
    "# Create submissions directory and save\n",
    "save_json(\"../submissions/lazy_v1.json\", results_v1)\n",
    "print(\"üíæ Saved to submissions/lazy_v1.json\")\n",
    "print(\"\\nüí° Tip: Keep all your submissions for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Submit to Leaderboard!\n",
    "\n",
    "This is it. The moment of truth. Let's get you on that leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ACTUAL SUBMISSION...\n",
      "‚úÖ Validated 100 results - format is correct!\n",
      "‚ùå Submission failed with status: 403\n",
      "Response: {\"message\":\"The security token included in the request is invalid.\"}\n",
      "\n",
      "üòÖ Something went wrong. Check the error message above.\n",
      "Common issues:\n",
      "  - Network timeout: Check your connection or VPN\n",
      "  - 403 error: You've hit the daily submission limit\n",
      "  - Format error: Run validation again\n",
      "\n",
      "Reach out on Teams if you need help!\n"
     ]
    }
   ],
   "source": [
    "# Now for real - submit to the leaderboard!\n",
    "# TODO for testers: Verify AWS endpoint is accessible from your network\n",
    "# If you get timeout errors, check with the org team on Teams\n",
    "\n",
    "print(\"üöÄ ACTUAL SUBMISSION...\")\n",
    "response = make_submission(results_v1, dry_run=False)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    print(\"\\nüéâ CONGRATULATIONS! You're on the leaderboard!\")\n",
    "    print(\"Go check your score at: [leaderboard URL]\")\n",
    "    print(\"(Yes, it's probably terrible. That's the point!)\")\n",
    "else:\n",
    "    print(\"\\nüòÖ Something went wrong. Check the error message above.\")\n",
    "    print(\"Common issues:\")\n",
    "    print(\"  - Network timeout: Check your connection or VPN\")\n",
    "    print(\"  - 403 error: You've hit the daily submission limit\")\n",
    "    print(\"  - Format error: Run validation again\")\n",
    "    print(\"\\nReach out on Teams if you need help!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Quick Win Celebration!\n",
    "\n",
    "If your submission worked, **CELEBRATE!** You just:\n",
    "- Went from idea to submission in ~15 minutes\n",
    "- Beat everyone still reading documentation\n",
    "- Got real feedback from a real system\n",
    "- Established your baseline for improvement\n",
    "\n",
    "**Your 1% score is a badge of honor** - it means you shipped! Share it in Teams with pride. Remember: every top competitor started with a terrible first submission.\n",
    "\n",
    "Now let's make it better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the GDSC API\n",
    "\n",
    "You sent your results to an endpoint just now with make_submission() function from utils.py. To learn more about the GDSC API and its documentation and endpoints, check out the GDSC API Documentation.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Real LLM Matcher - Load ALL Job Data\n",
    "\n",
    "Now let's do something that actually works: load ALL 200 job descriptions and ask the LLM to make informed recommendations.\n",
    "\n",
    "**Warning**: This will use ~50,000 tokens per API call (~$0.15-0.20). But at least it's based on real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded\n",
      "‚úÖ Mistral client ready with token tracking!\n"
     ]
    }
   ],
   "source": [
    "# Import our Mistral helper from Tutorial 2\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "\n",
    "# Load API key\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "\n",
    "# Check if we have the key\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è No API key found. This version needs Mistral API.\")\n",
    "    print(\"Check Tutorial 2 for setup instructions\")\n",
    "else:\n",
    "    print(\"‚úÖ API key loaded\")\n",
    "\n",
    "    # Implementation with actual token tracking (like Tutorial 2)\n",
    "    from strands import Agent\n",
    "    from strands.models.mistral import MistralModel\n",
    "\n",
    "    def call_mistral_with_metrics(prompt: str, model: str = \"mistral-small-latest\") -> dict:\n",
    "        \"\"\"Call Mistral API and track actual token usage and costs\"\"\"\n",
    "        mistral_model = MistralModel(\n",
    "            api_key=os.environ[\"MISTRAL_API_KEY\"],\n",
    "            model_id=model,\n",
    "            stream=False\n",
    "        )\n",
    "        agent = Agent(model=mistral_model)\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            response = agent(prompt)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Extract actual token counts and calculate real costs\n",
    "            result = {\n",
    "                \"content\": response.message['content'][0]['text'],\n",
    "                \"model\": model,\n",
    "                \"duration\": end_time - start_time,\n",
    "                \"input_tokens\": response.metrics.accumulated_usage['inputTokens'],\n",
    "                \"output_tokens\": response.metrics.accumulated_usage['outputTokens'],\n",
    "                \"total_tokens\": response.metrics.accumulated_usage['totalTokens']\n",
    "            }\n",
    "\n",
    "            # Calculate actual costs based on Mistral pricing\n",
    "            # mistral-small: $0.10 per 1M input tokens, $0.30 per 1M output tokens\n",
    "            input_cost = (result['input_tokens'] / 1_000_000) * 0.10\n",
    "            output_cost = (result['output_tokens'] / 1_000_000) * 0.30\n",
    "            result['total_cost'] = input_cost + output_cost\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API call failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"‚úÖ Mistral client ready with token tracking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_jobs():\n",
    "    \"\"\"Load all 200 job descriptions from the data folder.\"\"\"\n",
    "    import glob\n",
    "\n",
    "    jobs_data = {}\n",
    "    job_files = glob.glob(\"../data/jobs/*.md\")\n",
    "\n",
    "    print(f\"üìÇ Loading {len(job_files)} job files...\")\n",
    "\n",
    "    for filepath in sorted(job_files):\n",
    "        # Extract job ID from filename (e.g., job_acc_001.md -> job_acc_001)\n",
    "        job_id = filepath.split('/')[-1].replace('.md', '')\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            jobs_data[job_id] = content\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(jobs_data)} job descriptions\")\n",
    "\n",
    "    # Calculate approximate token count (rough estimate for planning)\n",
    "    total_chars = sum(len(content) for content in jobs_data.values())\n",
    "    approx_tokens = total_chars // 4  # Rough estimate: 1 token ‚âà 4 chars\n",
    "\n",
    "    print(f\"üìä Total content size: {total_chars:,} characters\")\n",
    "    print(f\"üéØ Estimated tokens: {approx_tokens:,}\")\n",
    "    print(f\"‚ö†Ô∏è  We'll see the ACTUAL token count after the API call\")\n",
    "\n",
    "    return jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_matcher_with_real_data():\n",
    "    \"\"\"\n",
    "    ACTUALLY smart matcher - loads real job data and asks LLM to recommend based on content.\n",
    "\n",
    "    Yes, this is expensive. But at least it's not blind guessing!\n",
    "    \"\"\"\n",
    "    # Load all job data\n",
    "    jobs_data = load_all_jobs()\n",
    "\n",
    "    # Build a massive prompt with all job descriptions\n",
    "    prompt = \"\"\"You are a career counselor for Brazilian youth interested in green jobs.\n",
    "\n",
    "Below are 200 actual job descriptions. Based on these REAL jobs, pick the 5 jobs that would be most suitable for the widest range of young Brazilian job seekers (ages 18-25).\n",
    "\n",
    "Choose jobs that:\n",
    "- Have reasonable entry requirements\n",
    "- Offer growth potential\n",
    "- Are in the green/sustainability sector\n",
    "- Are geographically distributed across Brazil\n",
    "\n",
    "Just respond with the 5 job IDs, one per line, like:\n",
    "job_xxx_001\n",
    "job_xxx_002\n",
    "job_xxx_003\n",
    "job_xxx_004\n",
    "job_xxx_005\n",
    "\n",
    "Here are all 200 job descriptions:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add all job descriptions to prompt\n",
    "    for job_id, content in jobs_data.items():\n",
    "        prompt += f\"\\n=== {job_id} ===\\n{content}\\n\"\n",
    "\n",
    "    print(f\"\\nüö® MASSIVE PROMPT ALERT!\")\n",
    "    print(f\"   Prompt length: {len(prompt):,} characters\")\n",
    "    print(f\"\\n   This is the 'brute force' approach - let's see the actual cost...\")\n",
    "\n",
    "    # Make the API call with token tracking\n",
    "    print(\"\\nü§ñ Making API call with ALL job data...\")\n",
    "    result = call_mistral_with_metrics(prompt)\n",
    "\n",
    "    if not result:\n",
    "        print(\"‚ùå API call failed!\")\n",
    "        return [], 0\n",
    "\n",
    "    # Show actual token usage and costs\n",
    "    print(f\"\\nüìä ACTUAL API USAGE:\")\n",
    "    print(f\"   Input tokens: {result['input_tokens']:,}\")\n",
    "    print(f\"   Output tokens: {result['output_tokens']:,}\")\n",
    "    print(f\"   Total tokens: {result['total_tokens']:,}\")\n",
    "    print(f\"   Duration: {result['duration']:.2f} seconds\")\n",
    "    print(f\"\\nüí∞ ACTUAL COST: ${result['total_cost']:.4f}\")\n",
    "    print(f\"   (Input: ${(result['input_tokens'] / 1_000_000) * 0.10:.4f} + Output: ${(result['output_tokens'] / 1_000_000) * 0.30:.4f})\")\n",
    "\n",
    "    print(f\"\\nüìù Response preview: {result['content'][:200]}...\")\n",
    "\n",
    "    # Parse the response to get job IDs\n",
    "    import re\n",
    "    selected_jobs = re.findall(r'job_\\w+_\\d{3}', result['content'])\n",
    "\n",
    "    print(f\"\\n‚úÖ LLM recommended these {len(selected_jobs)} universal jobs:\")\n",
    "    for job in selected_jobs:\n",
    "        print(f\"   - {job}\")\n",
    "\n",
    "    # Build results for all 100 personas - everyone gets one of these 5 jobs\n",
    "    results = []\n",
    "\n",
    "    for i in range(1, 101):\n",
    "        persona_id = f\"persona_{i:03}\"\n",
    "\n",
    "        # Give everyone one of the 5 recommended jobs (rotate through them)\n",
    "        if selected_jobs:\n",
    "            selected_job = selected_jobs[i % len(selected_jobs)]\n",
    "        else:\n",
    "            selected_job = \"job_001\"  # Fallback if parsing failed\n",
    "\n",
    "        results.append({\n",
    "            \"persona_id\": persona_id,\n",
    "            \"predicted_type\": \"jobs+trainings\",\n",
    "            \"jobs\": [{\n",
    "                \"job_id\": selected_job,\n",
    "                \"suggested_trainings\": []  # Keep it simple - no training suggestions\n",
    "            }]\n",
    "        })\n",
    "\n",
    "    return results, result['total_cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ SMART MATCHER WITH REAL JOB DATA\n",
      "============================================================\n",
      "üìÇ Loading 200 job files...\n",
      "‚úÖ Loaded 200 job descriptions\n",
      "üìä Total content size: 298,459 characters\n",
      "üéØ Estimated tokens: 74,614\n",
      "‚ö†Ô∏è  We'll see the ACTUAL token count after the API call\n",
      "\n",
      "üö® MASSIVE PROMPT ALERT!\n",
      "   Prompt length: 303,438 characters\n",
      "\n",
      "   This is the 'brute force' approach - let's see the actual cost...\n",
      "\n",
      "ü§ñ Making API call with ALL job data...\n",
      "Based on the job descriptions provided, here are the 5 most suitable jobs for young Brazilian job seekers (ages 18-25) interested in green jobs:\n",
      "\n",
      "job_acc_002\n",
      "job_cul_003\n",
      "job_des_003\n",
      "job_fib_003\n",
      "job_vis_002\n",
      "üìä ACTUAL API USAGE:\n",
      "   Input tokens: 54,703\n",
      "   Output tokens: 73\n",
      "   Total tokens: 54,776\n",
      "   Duration: 2.82 seconds\n",
      "\n",
      "üí∞ ACTUAL COST: $0.0055\n",
      "   (Input: $0.0055 + Output: $0.0000)\n",
      "\n",
      "üìù Response preview: Based on the job descriptions provided, here are the 5 most suitable jobs for young Brazilian job seekers (ages 18-25) interested in green jobs:\n",
      "\n",
      "job_acc_002\n",
      "job_cul_003\n",
      "job_des_003\n",
      "job_fib_003\n",
      "job_vi...\n",
      "\n",
      "‚úÖ LLM recommended these 5 universal jobs:\n",
      "   - job_acc_002\n",
      "   - job_cul_003\n",
      "   - job_des_003\n",
      "   - job_fib_003\n",
      "   - job_vis_002\n",
      "\n",
      "üìä Smart Matcher v2 Stats:\n",
      "  Total predictions: 100\n",
      "  Unique jobs recommended: 5 (rotated across all personas)\n",
      "  API calls: 1 (but a HUGE one)\n",
      "  ACTUAL cost: $0.0055\n",
      "  Expected score: Much better! (10-20%?)\n",
      "  Intelligence level: Knows the jobs, but not the personas\n",
      "\n",
      "üå± Green Computing Note:\n",
      "  We just used 54k tokens for ONE submission!\n",
      "  For 100 personas individually, that would be 5.4M tokens\n",
      "  This is why optimization matters - both for cost AND environment\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions with REAL data\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ SMART MATCHER WITH REAL JOB DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_v2, actual_cost = smart_matcher_with_real_data()\n",
    "\n",
    "print(\"\\nüìä Smart Matcher v2 Stats:\")\n",
    "print(f\"  Total predictions: {len(results_v2)}\")\n",
    "print(f\"  Unique jobs recommended: 5 (rotated across all personas)\")\n",
    "print(f\"  API calls: 1 (but a HUGE one)\")\n",
    "print(f\"  ACTUAL cost: ${actual_cost:.4f}\")\n",
    "print(f\"  Expected score: Much better! (10-20%?)\")\n",
    "print(f\"  Intelligence level: Knows the jobs, but not the personas\")\n",
    "\n",
    "print(\"\\nüå± Green Computing Note:\")\n",
    "print(f\"  We just used 54k tokens for ONE submission!\")\n",
    "print(f\"  For 100 personas individually, that would be 5.4M tokens\")\n",
    "print(f\"  This is why optimization matters - both for cost AND environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validated 100 results - format is correct!\n",
      "\n",
      "üöÄ Submitting Smart Matcher v2...\n",
      "‚úÖ Validated 100 results - format is correct!\n",
      "‚ùå Submission failed with status: 403\n",
      "Response: {\"error\": \"Submission limit of 2 reached for today for this team\"}\n"
     ]
    }
   ],
   "source": [
    "# Validate, save, and submit v2\n",
    "validate_submission_format(results_v2)\n",
    "save_json(\"../submissions/smart_v2.json\", results_v2)\n",
    "\n",
    "print(\"\\nüöÄ Submitting Smart Matcher v2...\")\n",
    "response = make_submission(results_v2)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    print(\"\\nüìà This should score MUCH better than the lazy matcher!\")\n",
    "    print(\"But look at that cost... time to optimize in Tutorial 4!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Homework - Real Exercises This Time!\n",
    "\n",
    "### Exercise 1: Investigate the LLM's Job Choices\n",
    "```python\n",
    "def analyze_llm_job_picks():\n",
    "    \"\"\"\n",
    "    In the smart matcher, the LLM picked specific jobs (maybe job_cul_003-007?).\n",
    "    Your task: Actually READ those job files and figure out WHY.\n",
    "    \n",
    "    Questions to answer:\n",
    "    - What do these jobs have in common?\n",
    "    - Are they really entry-level friendly?\n",
    "    - Do they mention sustainability/green aspects?\n",
    "    - What locations do they cover?\n",
    "    \n",
    "    Bonus: Compare them to 5 random other jobs. What's different?\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # The jobs the LLM picked (update this based on your run!)\n",
    "    llm_picks = [\"job_cul_003\", \"job_cul_004\", \"job_cul_005\", \"job_cul_006\", \"job_cul_007\"]\n",
    "    \n",
    "    # Load and analyze the job files\n",
    "    for job_id in llm_picks:\n",
    "        filepath = f\"../data/jobs/{job_id}.md\"\n",
    "        # Your analysis code here\n",
    "        pass\n",
    "    \n",
    "    # Compare with random jobs\n",
    "    # What patterns do you see?\n",
    "```\n",
    "\n",
    "### Exercise 2: Cost vs Randomness\n",
    "```python\n",
    "def calculate_random_score():\n",
    "    \"\"\"\n",
    "    If you randomly assigned 5 different jobs to each persona,\n",
    "    what would your expected score be?\n",
    "    \n",
    "    Hint: Better than 1% (not everyone gets the same job)\n",
    "          Worse than 20% (no intelligence applied)\n",
    "    \n",
    "    Assumptions:\n",
    "    - 200 jobs total\n",
    "    - Each persona gets 1 random job\n",
    "    - What's the probability of a random match?\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Think about: P(correct domain) * P(correct seniority) * P(location match) * ...\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Exercise 3: Smarter Lazy Matcher\n",
    "```python\n",
    "def lazy_matcher_v2():\n",
    "    \"\"\"\n",
    "    Improve the lazy matcher without using any API calls!\n",
    "    \n",
    "    Ideas:\n",
    "    - Assign different jobs based on persona_id\n",
    "      e.g., persona_001-020 get job_001, persona_021-040 get job_002\n",
    "    - Or use modulo: persona_id % 10 maps to different jobs\n",
    "    - Test if this improves your score over everyone getting job_001\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Your implementation here\n",
    "    # Remember: No API calls allowed in this version!\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "### Exercise 4: Token Economics \n",
    "```python\n",
    "def compare_api_strategies():\n",
    "    \"\"\"\n",
    "    Calculate the cost of these approaches:\n",
    "    \n",
    "    a) One API call with ALL jobs (what we did): ~$0.006\n",
    "    b) 100 API calls (one per persona) with 10 jobs each: ~$???\n",
    "    c) 10 API calls with 10 personas each batched: ~$???\n",
    "    d) Smart caching: Process jobs once, reuse for all personas: ~$???\n",
    "    \n",
    "    Which is most cost-effective? Show your work!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constants (from Mistral pricing)\n",
    "    COST_PER_M_INPUT = 0.10  # Small model\n",
    "    COST_PER_M_OUTPUT = 0.30\n",
    "    AVG_JOB_TOKENS = 300\n",
    "    AVG_PERSONA_TOKENS = 200\n",
    "    AVG_OUTPUT_TOKENS = 100\n",
    "    \n",
    "    # Calculate costs for each approach\n",
    "    # Your code here\n",
    "    \n",
    "    pass\n",
    "```\n",
    "\n",
    "### Exercise 5: Submission Analysis (Advanced)\n",
    "```python\n",
    "def analyze_my_submissions():\n",
    "    \"\"\"\n",
    "    Load your saved submissions and compare them:\n",
    "    - How many unique jobs does each recommend?\n",
    "    - What's the overlap between submissions?\n",
    "    - Can you predict which will score better?\n",
    "    \n",
    "    Bonus: Write a function to merge two submissions,\n",
    "    taking the best predictions from each!\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Load submissions/lazy_v1.json and submissions/smart_v2.json\n",
    "    # Compare and analyze\n",
    "    \n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must Do Today:\n",
    "1. **Submit at least 3 different approaches** (you've already done 2!)\n",
    "2. **Complete Exercise 1** - Understand WHY the LLM chose those jobs\n",
    "3. **Track your scores** in a spreadsheet or text file\n",
    "4. **Share your scores in Teams** - embrace the terrible scores!\n",
    "\n",
    "### Should Do:\n",
    "1. **Complete Exercise 3** - Can you beat 1% without API calls?\n",
    "2. **Complete Exercise 4** - Understand the cost tradeoffs\n",
    "3. **Peek at src/utils.py** - Use Copilot to understand the functions\n",
    "\n",
    "### Could Do:\n",
    "1. **Read more job files manually** - What patterns do you notice?\n",
    "2. **Think about Tutorial 4** - How would you get to 40% accuracy?\n",
    "3. **Help someone else** - Share tips in the Teams channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Tracking Summary\n",
    "\n",
    "Let's be real about costs:\n",
    "\n",
    "| Approach | API Calls | Actual Cost | Expected Score | Key Learning |\n",
    "|----------|-----------|-------------|----------------|--------------|\n",
    "| Lazy v1 | 0 | $0.00 | ~1% | Baseline - no intelligence |\n",
    "| Smart with Real Data | 1 | ~$0.05-0.10 | ~10-20% | Understanding jobs helps a lot |\n",
    "| Tutorial 4 | ? | ? | ? | We'll explore smarter approaches |\n",
    "\n",
    "### Key Insights So Far:\n",
    "\n",
    "1. **Zero to hero for cheap** - One API call with all job data gives huge improvement\n",
    "2. **The brute force approach works** - But sending 50k tokens isn't elegant\n",
    "3. **Rotation isn't personalization** - We're giving everyone the same 5 jobs\n",
    "4. **Room for improvement** - What if we actually understood each persona?\n",
    "\n",
    "### Ideas for Optimization (Tutorial 4):\n",
    "\n",
    "- Talk to individual personas to understand their needs\n",
    "- Cache and reuse information intelligently  \n",
    "- Use smaller, focused prompts instead of massive ones\n",
    "- Build actual matching logic based on skills and constraints\n",
    "\n",
    "### The Real Challenge:\n",
    "\n",
    "It's not about getting 100% accuracy. It's about finding the sweet spot:\n",
    "- **Good enough accuracy** \n",
    "- **Reasonable cost**\n",
    "- **Fast execution**\n",
    "\n",
    "This is exactly what real consultants deal with: balancing quality, cost, and time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "### Tutorial 4: Building Real LLM-Based Matching\n",
    "\n",
    "Now that you understand the basics, Tutorial 4 will teach you:\n",
    "- **AI Agents**: Conversation agents that actually talk to personas\n",
    "- **Information Extraction**: Getting structured data from conversations\n",
    "- **Intelligent Matching**: Real skill-based job matching\n",
    "- **Cost Optimization**: Being smart about API usage\n",
    "\n",
    "But for now, celebrate! You're on the leaderboard. You shipped code. You're ahead of 90% of participants who are still planning.\n",
    "\n",
    "**Remember**: \n",
    "- Bad code that ships > Perfect code that doesn't\n",
    "- Your score will improve dramatically in Tutorial 4\n",
    "- The real learning happens through iteration\n",
    "\n",
    "See you in Tutorial 4, where we'll build something actually intelligent! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSC-8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
