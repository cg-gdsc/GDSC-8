{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Your First Submission - Get on the Leaderboard in 15 Minutes!\n",
    "\n",
    "Alright, forget the fancy AI stuff for now. Let's just get you on that leaderboard.\n",
    "\n",
    "## Today's mission\n",
    "\n",
    "- Submit SOMETHING that works (even if it's terrible) \n",
    "- Learn why notebooks suck for production (and how to fix it)\n",
    "- Write actual tests (yes, even for a hackathon)\n",
    "- Understand the scoring by failing fast\n",
    "\n",
    "**Real talk**: Your first submission will score terribly. Ship it anyway. You learn more from one bad submission than from endless planning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Project Structure (The Right Way‚Ñ¢)\n",
    "\n",
    "Remember Tutorial 2 where we just dumped everything in the notebook? Yeah, that doesn't scale. Let's be slightly more professional.\n",
    "\n",
    "**Our new structure:**\n",
    "```\n",
    "GDSC-8/\n",
    "‚îú‚îÄ‚îÄ src/           # Reusable code goes here\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils.py   # Submission, validation, I/O utilities\n",
    "‚îú‚îÄ‚îÄ tests/         # Yes, we're writing tests\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n",
    "‚îî‚îÄ‚îÄ tutorials/     # Notebooks for exploration\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- Notebooks = great for exploration, terrible for production\n",
    "- Modules = testable, reusable, version-controllable\n",
    "- Tests = confidence that your code actually works\n",
    "\n",
    "Let's import our utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports - the professional way\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path so we can import from src/\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our utilities\n",
    "from src.utils import (\n",
    "    send_results, \n",
    "    save_json, \n",
    "    read_json,\n",
    "    validate_submission_format,\n",
    "    sanity_check\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Imported from src/ like a pro\")\n",
    "print(\"üí° Production tip: Keep reusable code in modules, not notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Sanity Check\n",
    "\n",
    "Before we submit anything, let's make sure our AWS connection works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API connection\n",
    "if sanity_check():\n",
    "    print(\"\\nüéØ Ready to submit!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Fix your AWS credentials first!\")\n",
    "    print(\"Check Tutorial 1 for setup instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The World's Laziest Matcher\n",
    "\n",
    "Let's build the absolute minimum viable submission. Everyone gets the same job. Zero intelligence. Zero API calls. Maximum speed.\n",
    "\n",
    "**Why start here?**\n",
    "- Understand the submission format\n",
    "- Test the full pipeline\n",
    "- Get that psychological win of being on the leaderboard\n",
    "- Establish a baseline (spoiler: it'll be bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_matcher_v1():\n",
    "    \"\"\"\n",
    "    The laziest possible solution that still works.\n",
    "    Everyone gets job_001. No personalization. No intelligence.\n",
    "    \n",
    "    This is your baseline - everything else should beat this.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(1, 101):\n",
    "        results.append({\n",
    "            \"persona_id\": f\"persona_{i:03}\",\n",
    "            \"predicted_type\": \"jobs+trainings\",\n",
    "            \"jobs\": [\n",
    "                {\n",
    "                    \"job_id\": \"job_001\",\n",
    "                    \"suggested_trainings\": []  # No training suggestions\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate our terrible predictions\n",
    "results_v1 = lazy_matcher_v1()\n",
    "\n",
    "print(\"üìä Lazy Matcher v1 Stats:\")\n",
    "print(f\"  Predictions: {len(results_v1)}\")\n",
    "print(f\"  Unique jobs recommended: 1\")\n",
    "print(f\"  API calls made: 0\")\n",
    "print(f\"  Cost: $0.00\")\n",
    "print(f\"  Expected score: Terrible\")\n",
    "print(f\"  Time to implement: 30 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Validate Before Submitting\n",
    "\n",
    "**Pro tip**: Always validate your format before submitting. Catching errors locally is free. Debugging failed submissions is painful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always validate before submitting!\n",
    "try:\n",
    "    validate_submission_format(results_v1)\n",
    "    print(\"‚úÖ Format is valid! Ready to submit\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Format error: {e}\")\n",
    "    print(\"Fix this before submitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Your Work\n",
    "\n",
    "**Best practice**: Always save your submissions. You'll want to compare different approaches later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions directory and save\n",
    "save_json(\"../submissions/lazy_v1.json\", results_v1)\n",
    "print(\"üíæ Saved to submissions/lazy_v1.json\")\n",
    "print(\"\\nüí° Tip: Keep all your submissions for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Submit to Leaderboard!\n",
    "\n",
    "This is it. The moment of truth. Let's get you on that leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run first - always test before the real thing\n",
    "print(\"üîç Dry run (validation only)...\")\n",
    "response = send_results(results_v1, dry_run=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for real - submit to the leaderboard!\n",
    "print(\"üöÄ ACTUAL SUBMISSION...\")\n",
    "response = send_results(results_v1, dry_run=False)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    print(\"\\nüéâ CONGRATULATIONS! You're on the leaderboard!\")\n",
    "    print(\"Go check your score at: [leaderboard URL]\")\n",
    "    print(\"(Yes, it's probably terrible. That's the point!)\")\n",
    "else:\n",
    "    print(\"\\nüòÖ Something went wrong. Check the error message above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Quick Iteration - Random Matcher\n",
    "\n",
    "Your lazy matcher probably scored around 1-5%. Let's try something slightly smarter: random assignments. Still no API calls, but at least there's variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_matcher_v2():\n",
    "    \"\"\"\n",
    "    Slightly less lazy - random assignments.\n",
    "    \n",
    "    Some personas are too young (awareness).\n",
    "    Some need training first.\n",
    "    Most get random jobs.\n",
    "    \"\"\"\n",
    "    # Sample job and training IDs (in reality, we have 200 jobs and 467 trainings)\n",
    "    job_ids = [f\"job_{i:03}\" for i in range(1, 21)]  # Use first 20 jobs\n",
    "    training_ids = [f\"training_{i:03}\" for i in range(1, 31)]  # First 30 trainings\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(1, 101):\n",
    "        persona_id = f\"persona_{i:03}\"\n",
    "        \n",
    "        # 10% chance of being too young (awareness)\n",
    "        if random.random() < 0.1:\n",
    "            results.append({\n",
    "                \"persona_id\": persona_id,\n",
    "                \"predicted_type\": \"awareness\",\n",
    "                \"predicted_items\": \"too_young\"\n",
    "            })\n",
    "        \n",
    "        # 20% need training only\n",
    "        elif random.random() < 0.3:\n",
    "            results.append({\n",
    "                \"persona_id\": persona_id,\n",
    "                \"predicted_type\": \"trainings_only\",\n",
    "                \"trainings\": random.sample(training_ids, k=min(3, len(training_ids)))\n",
    "            })\n",
    "        \n",
    "        # 70% get job recommendations\n",
    "        else:\n",
    "            # Pick 1-3 random jobs\n",
    "            num_jobs = random.randint(1, 3)\n",
    "            selected_jobs = random.sample(job_ids, k=min(num_jobs, len(job_ids)))\n",
    "            \n",
    "            jobs = []\n",
    "            for job_id in selected_jobs:\n",
    "                # Sometimes suggest trainings (50% chance)\n",
    "                if random.random() < 0.5:\n",
    "                    suggested = random.sample(training_ids, k=random.randint(0, 2))\n",
    "                else:\n",
    "                    suggested = []\n",
    "                    \n",
    "                jobs.append({\n",
    "                    \"job_id\": job_id,\n",
    "                    \"suggested_trainings\": suggested\n",
    "                })\n",
    "            \n",
    "            results.append({\n",
    "                \"persona_id\": persona_id,\n",
    "                \"predicted_type\": \"jobs+trainings\",\n",
    "                \"jobs\": jobs\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate random predictions\n",
    "results_v2 = random_matcher_v2()\n",
    "\n",
    "# Analyze the distribution\n",
    "types = [r['predicted_type'] for r in results_v2]\n",
    "print(\"üìä Random Matcher v2 Stats:\")\n",
    "print(f\"  Total predictions: {len(results_v2)}\")\n",
    "print(f\"  Jobs+trainings: {types.count('jobs+trainings')}\")\n",
    "print(f\"  Trainings only: {types.count('trainings_only')}\")\n",
    "print(f\"  Awareness: {types.count('awareness')}\")\n",
    "print(f\"  API calls: 0\")\n",
    "print(f\"  Cost: $0.00\")\n",
    "print(f\"  Expected score: Still bad, but better than v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and save v2\n",
    "validate_submission_format(results_v2)\n",
    "save_json(\"../submissions/random_v2.json\", results_v2)\n",
    "\n",
    "# Submit v2\n",
    "print(\"\\nüöÄ Submitting Random Matcher v2...\")\n",
    "response = send_results(results_v2)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    print(\"\\nüìà Check if you improved! Even 1% better is progress.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: One API Call Wonder\n",
    "\n",
    "Alright, let's use ONE Mistral API call to be slightly smarter. We'll ask the LLM to pick versatile jobs that work for many personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our Mistral helper from Tutorial 2\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# Load API key\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "\n",
    "# Check if we have the key\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è No API key found. This version needs Mistral API.\")\n",
    "    print(\"Skipping smart matcher - use random matcher instead.\")\n",
    "else:\n",
    "    print(\"‚úÖ API key loaded\")\n",
    "    \n",
    "    # Quick implementation of call_mistral for this tutorial\n",
    "    from strands.agent import Agent\n",
    "    from strands.models.mistral import MistralModel\n",
    "    \n",
    "    def quick_mistral_call(prompt: str) -> str:\n",
    "        \"\"\"Super simple Mistral call - just get a response.\"\"\"\n",
    "        model = MistralModel(\n",
    "            api_key=os.environ[\"MISTRAL_API_KEY\"],\n",
    "            model_id=\"mistral-small-latest\",\n",
    "            stream=False\n",
    "        )\n",
    "        agent = Agent(model=model)\n",
    "        response = agent(prompt)\n",
    "        return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_lazy_matcher_v3():\n",
    "    \"\"\"\n",
    "    Use ONE API call to pick good default jobs.\n",
    "    Then assign them semi-randomly.\n",
    "    \n",
    "    Still lazy, but at least the jobs might be relevant.\n",
    "    \"\"\"\n",
    "    if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "        print(\"No API key - falling back to random\")\n",
    "        return random_matcher_v2()\n",
    "    \n",
    "    # ONE API call to pick versatile jobs\n",
    "    prompt = \"\"\"\n",
    "    We have 200 green jobs in Brazil (job_001 to job_200) for young people.\n",
    "    Pick 5 job IDs that would work for beginners interested in sustainability.\n",
    "    Just list the IDs like: job_001, job_002, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Making our ONE API call...\")\n",
    "    response = quick_mistral_call(prompt)\n",
    "    print(f\"Response: {response[:100]}...\")\n",
    "    \n",
    "    # Parse job IDs from response (with fallback)\n",
    "    import re\n",
    "    job_matches = re.findall(r'job_\\d{3}', response)\n",
    "    \n",
    "    if not job_matches:\n",
    "        # Fallback if parsing fails\n",
    "        job_matches = [f\"job_{i:03}\" for i in [1, 5, 10, 15, 20]]\n",
    "        print(\"‚ö†Ô∏è Couldn't parse response, using defaults\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(job_matches)} job recommendations\")\n",
    "    \n",
    "    # Similarly for trainings (or just use defaults)\n",
    "    training_ids = [f\"training_{i:03}\" for i in range(1, 11)]\n",
    "    \n",
    "    # Now distribute these \"smart\" picks across personas\n",
    "    results = []\n",
    "    \n",
    "    for i in range(1, 101):\n",
    "        persona_id = f\"persona_{i:03}\"\n",
    "        \n",
    "        # Still use some randomness for variety\n",
    "        if i % 10 == 0:  # Every 10th persona\n",
    "            # Awareness (too young)\n",
    "            results.append({\n",
    "                \"persona_id\": persona_id,\n",
    "                \"predicted_type\": \"awareness\",\n",
    "                \"predicted_items\": \"too_young\"\n",
    "            })\n",
    "        elif i % 5 == 0:  # Every 5th persona\n",
    "            # Training only\n",
    "            results.append({\n",
    "                \"persona_id\": persona_id,\n",
    "                \"predicted_type\": \"trainings_only\",\n",
    "                \"trainings\": random.sample(training_ids, k=2)\n",
    "            })\n",
    "        else:\n",
    "            # Jobs from our \"smart\" selection\n",
    "            selected_job = random.choice(job_matches)\n",
    "            results.append({\n",
    "                \"persona_id\": persona_id,\n",
    "                \"predicted_type\": \"jobs+trainings\",\n",
    "                \"jobs\": [\n",
    "                    {\n",
    "                        \"job_id\": selected_job,\n",
    "                        \"suggested_trainings\": random.sample(training_ids, k=1)\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate \"smart\" predictions\n",
    "results_v3 = smart_lazy_matcher_v3()\n",
    "\n",
    "print(\"\\nüìä Smart Lazy Matcher v3 Stats:\")\n",
    "print(f\"  Total predictions: {len(results_v3)}\")\n",
    "print(f\"  API calls: 1\")\n",
    "print(f\"  Estimated cost: ~$0.0001\")\n",
    "print(f\"  Expected score: Slightly better?\")\n",
    "print(f\"  Intelligence level: Barely any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate, save, and submit v3\n",
    "validate_submission_format(results_v3)\n",
    "save_json(\"../submissions/smart_lazy_v3.json\", results_v3)\n",
    "\n",
    "print(\"\\nüöÄ Submitting Smart Lazy Matcher v3...\")\n",
    "response = send_results(results_v3)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    print(\"\\nüí° Even tiny improvements count!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Testing Your Code\n",
    "\n",
    "**Production mindset**: Even competition code needs tests. They save debugging time and catch silly mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our test suite\n",
    "!cd .. && python -m pytest tests/test_utils.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with coverage to see what we're testing\n",
    "!cd .. && python -m pytest tests/ --cov=src --cov-report=term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: The Psychology of Progress\n",
    "\n",
    "### What we just accomplished:\n",
    "\n",
    "1. **You're on the leaderboard** - That's huge! Most people are still planning.\n",
    "2. **You understand the system** - Format, submission process, scoring.\n",
    "3. **You have a baseline** - Everything from here is improvement.\n",
    "4. **You built infrastructure** - Tests, utilities, proper structure.\n",
    "\n",
    "### Your scores so far:\n",
    "\n",
    "| Version | API Calls | Cost | Expected Score | Lesson |\n",
    "|---------|-----------|------|----------------|--------|\n",
    "| Lazy v1 | 0 | $0.00 | ~1% | Baseline established |\n",
    "| Random v2 | 0 | $0.00 | ~3% | Variety helps |\n",
    "| Smart Lazy v3 | 1 | ~$0.0001 | ~5% | Tiny intelligence helps |\n",
    "\n",
    "### Why starting simple matters:\n",
    "\n",
    "- **Momentum > Perfection**: You've already submitted 3 times while others are still reading docs\n",
    "- **Learning by doing**: Each submission teaches you something\n",
    "- **Fail fast, improve faster**: Your 4th submission will be way better\n",
    "- **Psychological wins**: Being on the leaderboard motivates you to improve\n",
    "\n",
    "### Common mistakes to avoid:\n",
    "\n",
    "1. **Over-engineering v1**: Don't build a complex system before understanding the problem\n",
    "2. **Ignoring tests**: That one typo can waste hours of compute\n",
    "3. **Not saving submissions**: You'll want to analyze what worked later\n",
    "4. **Perfectionism**: Ship garbage, then iterate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Homework\n",
    "\n",
    "### Must do:\n",
    "1. Submit at least 3 different approaches today\n",
    "2. Track your scores in a spreadsheet\n",
    "3. Share your (terrible) scores in the Teams channel - embrace it!\n",
    "\n",
    "### Should do:\n",
    "1. Write a test for your matching logic\n",
    "2. Create `src/matchers.py` for your better algorithms\n",
    "3. Try using 2-3 API calls strategically\n",
    "\n",
    "### Could do:\n",
    "1. Analyze which personas you're getting wrong\n",
    "2. Look at the job/training data files\n",
    "3. Start thinking about Tutorial 4's intelligent approach\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Tracking Summary\n",
    "\n",
    "Let's be real about costs:\n",
    "\n",
    "| Approach | API Calls | Total Cost | Score | Cost per % |\n",
    "|----------|-----------|------------|-------|------------|\n",
    "| Lazy v1 | 0 | $0.00 | ~1% | $0.00 |\n",
    "| Random v2 | 0 | $0.00 | ~3% | $0.00 |\n",
    "| Smart Lazy v3 | 1 | ~$0.0001 | ~5% | $0.00002 |\n",
    "| Tutorial 4 (preview) | ~200 | ~$0.20 | ~40% | $0.005 |\n",
    "\n",
    "**Key insight**: Going from 0% to 5% costs almost nothing. Going from 40% to 60% will cost way more.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "### Tutorial 4: Building Real LLM-Based Matching\n",
    "\n",
    "Now that you understand the basics, Tutorial 4 will teach you:\n",
    "- **AI Agents**: Conversation agents that actually talk to personas\n",
    "- **Information Extraction**: Getting structured data from conversations\n",
    "- **Intelligent Matching**: Real skill-based job matching\n",
    "- **Cost Optimization**: Being smart about API usage\n",
    "\n",
    "But for now, celebrate! You're on the leaderboard. You shipped code. You're ahead of 90% of participants who are still planning.\n",
    "\n",
    "**Remember**: \n",
    "- Bad code that ships > Perfect code that doesn't\n",
    "- Your score will improve dramatically in Tutorial 4\n",
    "- The real learning happens through iteration\n",
    "\n",
    "See you in Tutorial 4, where we'll build something actually intelligent! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}