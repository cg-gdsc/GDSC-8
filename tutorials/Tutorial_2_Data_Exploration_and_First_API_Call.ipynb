{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial 2: Data Exploration and Your First API Call\n\nAlright, let's get our hands dirty with some actual data and API calls. \n\n## What we're doing today\n\n- Download the GDSC 8 dataset (jobs + trainings from Brazil)\n- Make our first Mistral API call (and not go broke doing it)\n- Understand why tokens matter (spoiler: they cost money)\n- Use LLMs to filter data instead of writing regex hell\n\n**Reality check**: This is about building AI agents that help people find green jobs in Brazil. Cool mission, but also we're in a competition, so let's be smart about costs and performance.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding the Challenge Data\n\n### The Mission (kinda cool actually)\nWe're helping young people in Brazil find green jobs. UNICEF partnership, climate action, meaningful careers - the whole deal. But here's the thing: we need to build AI agents that can sift through job descriptions and training programs, match them to people's profiles, and do it efficiently.\n\n### Let's grab the data\nTime to download some files from S3:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GDSC 8 dataset\n",
    "!aws s3 cp s3://gdsc25test/ . --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "After this runs, you'll have a `data` directory with:\n- **`jobs/`** - 200 job postings\n- **`trainings/`** - 497 training programs\n\n### Quick math reality check\n697 items Ã— however many personas we need to match = potentially expensive if we're not careful with API calls.\n\nThis is where being smart about it pays off. Literally."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's see what we're working with\nimport os\nfrom pathlib import Path\n\n# Count files and get basic statistics\njobs_dir = Path('data/jobs')\ntrainings_dir = Path('data/trainings')\n\njob_files = list(jobs_dir.glob('*.md')) if jobs_dir.exists() else []\ntraining_files = list(trainings_dir.glob('*.md')) if trainings_dir.exists() else []\n\nprint(f\"Dataset Overview:\")\nprint(f\"Jobs: {len(job_files)}\")\nprint(f\"Trainings: {len(training_files)}\")\nprint(f\"Total items: {len(job_files) + len(training_files)}\")\n\n# TODO: Add breakdowns by domain, seniority level, location\n# (Author note: this would be useful for understanding what we're dealing with)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper function to peek at files\nfrom IPython.display import Markdown, display\n\ndef display_markdown_file(path: str) -> None:\n    \"\"\"Display a markdown file in Jupyter - nothing fancy\"\"\"\n    p = Path(path)\n    if not p.exists():\n        print(f\"File not found: {p}\")\n        return\n    content = p.read_text(encoding='utf-8', errors='ignore')\n    display(Markdown(content))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Let's look at a job posting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample job\n",
    "if job_files:\n",
    "    display_markdown_file(job_files[0])\n",
    "else:\n",
    "    print(\"No job files found. Make sure you've downloaded the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### And a training program"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample training\n",
    "if training_files:\n",
    "    display_markdown_file(training_files[0])\n",
    "else:\n",
    "    print(\"No training files found. Make sure you've downloaded the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What you'll notice\n\nBoth jobs and trainings have:\n- **Overview/Description** \n- **Location** (this matters for matching)\n- **Prerequisites** (skills, experience levels)\n- **Outcomes** (for trainings)\n\nBut here's the kicker: they're not consistently formatted. Some use different headers, different structures, different language. Your LLM solution needs to handle this chaos gracefully.\n\nThis is why we can't just use regex or simple parsing - we need something smarter.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Your First Mistral API Call\n\nTime to get our hands dirty with the actual AI part."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup time\n# !pip install python-dotenv mistralai\n\nimport os\nimport dotenv\nimport time\nfrom mistralai import Mistral\n\n# Load your API key from .env file\ndotenv.load_dotenv()\n\n# Check if we're good to go\nif not os.getenv(\"MISTRAL_API_KEY\"):\n    print(\"âŒ No MISTRAL_API_KEY found!\")\n    print(\"Create a .env file with your API key\")\nelse:\n    print(\"âœ… API key found, we're ready to roll\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Quick Token 101\n\n**Tokens are how LLMs \"see\" text**. Think of them as word chunks:\n- \"Hello world!\" â‰ˆ 3 tokens\n- \"The\" = 1 token, \"ing\" = 1 token\n- Roughly 1 token â‰ˆ 0.75 English words\n\n**Why you care**:\n- Tokens = money (API costs)\n- Tokens = speed (more tokens = slower)\n- Tokens = limits (models have max context windows)\n\n**Golden rule**: Start with the smallest/cheapest model that can do the job. Scale up only if needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick token estimator (rough but useful)\ndef estimate_tokens(text: str) -> int:\n    \"\"\"Ballpark token count - good enough for cost estimates\"\"\"\n    words = len(text.split())\n    return int(words / 0.75)\n\n# Test it out\ntest_prompt = \"Analyze this job posting and extract the key requirements.\"\nprint(f\"Prompt: '{test_prompt}'\")\nprint(f\"Estimated tokens: {estimate_tokens(test_prompt)}\")\n\n# TODO: Use actual Mistral tokenizer for precise counts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model Comparison - The Money Talk\n\n| Model | Size | Cost/1M tokens | Best for |\n|-------|------|---------------|----------|\n| mistral-small-latest | Small | ~$0.20 | Classification, simple extraction |\n| mistral-large-latest | Large | ~$3.00 | Complex reasoning, detailed analysis |\n\n*TODO: Get actual current pricing - these are rough estimates*\n\n**Real talk**: For most filtering/classification tasks, the small model is plenty good and 15x cheaper. Only use the big guns when you really need them.\n\n> Author note: Show concrete examples with actual token counts and costs side-by-side"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# API wrapper with timing and cost tracking\nclient = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n\ndef call_mistral(prompt: str, model: str = \"mistral-small-latest\") -> dict:\n    \"\"\"Call Mistral API and track what it costs us\"\"\"\n    start_time = time.time()\n    \n    try:\n        response = client.chat.complete(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        end_time = time.time()\n        \n        # Extract useful info\n        result = {\n            \"content\": response.choices[0].message.content,\n            \"model\": model,\n            \"duration\": end_time - start_time,\n            \"input_tokens\": response.usage.prompt_tokens,\n            \"output_tokens\": response.usage.completion_tokens,\n            \"total_tokens\": response.usage.total_tokens\n        }\n        \n        return result\n        \n    except Exception as e:\n        print(f\"âŒ API call failed: {e}\")\n        return None\n\nprint(\"âœ… Mistral client ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### First API call - let's do this"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple test to make sure everything works\ntest_prompt = \"\"\"What's a 'green job'? Keep it short and practical.\"\"\"\n\nprint(\"ðŸš€ First API call...\")\nresult = call_mistral(test_prompt, \"mistral-small-latest\")\n\nif result:\n    print(f\"\\nðŸ“Š Stats:\")\n    print(f\"Model: {result['model']}\")\n    print(f\"Time: {result['duration']:.2f} seconds\")\n    print(f\"Input tokens: {result['input_tokens']}\")\n    print(f\"Output tokens: {result['output_tokens']}\")\n    print(f\"Total tokens: {result['total_tokens']}\")\n    \n    # TODO: Calculate actual cost\n    # estimated_cost = (result['total_tokens'] / 1_000_000) * 0.20\n    # print(f\"Estimated cost: ${estimated_cost:.6f}\")\n    \n    print(f\"\\nðŸ’¬ Response:\")\n    print(result['content'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model comparison experiment\n\nLet's see the difference between small and large models on the same task:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex analysis prompt for comparison\n",
    "analysis_prompt = \"\"\"Analyze this job description and extract:\n",
    "1. Required skills (list)\n",
    "2. Seniority level (basic/intermediate/advanced)\n",
    "3. Location requirements\n",
    "4. Whether it's related to sustainability/green jobs\n",
    "\n",
    "Job Description:\n",
    "# Renewable Energy Systems Engineer\n",
    "## Overview\n",
    "Join our mission to accelerate Brazil's transition to clean energy! We're seeking a systems engineer to design and optimize solar and wind energy installations across SÃ£o Paulo region.\n",
    "\n",
    "## Requirements\n",
    "- Engineering degree (Electrical, Mechanical, or Environmental)\n",
    "- 2-3 years experience in renewable energy projects\n",
    "- Proficiency in MATLAB/Simulink and AutoCAD\n",
    "- Portuguese fluency required\n",
    "- Willingness to travel within SÃ£o Paulo state\n",
    "\n",
    "Format your response as structured JSON.\"\"\"\n",
    "\n",
    "print(\"ðŸ”¬ Running model comparison experiment...\\n\")\n",
    "\n",
    "# Test with small model\n",
    "print(\"Testing mistral-small-latest:\")\n",
    "result_small = call_mistral(analysis_prompt, \"mistral-small-latest\")\n",
    "\n",
    "if result_small:\n",
    "    print(f\"Duration: {result_small['duration']:.2f}s | Tokens: {result_small['total_tokens']}\")\n",
    "    print(\"Response:\")\n",
    "    print(result_small['content'][:200] + \"...\" if len(result_small['content']) > 200 else result_small['content'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test with large model\n",
    "print(\"Testing mistral-large-latest:\")\n",
    "result_large = call_mistral(analysis_prompt, \"mistral-large-latest\")\n",
    "\n",
    "if result_large:\n",
    "    print(f\"Duration: {result_large['duration']:.2f}s | Tokens: {result_large['total_tokens']}\")\n",
    "    print(\"Response:\")\n",
    "    print(result_large['content'][:200] + \"...\" if len(result_large['content']) > 200 else result_large['content'])\n",
    "\n",
    "# TODO: Add cost comparison calculation\n",
    "# TODO: Add quality assessment framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Using LLMs for Data Filtering\n\n### The problem\n697 items in our dataset. We need to categorize them efficiently without manually reading everything.\n\n### The solution\nUse LLMs to do the boring classification work for us."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a training example for filtering\n",
    "sample_training_path = None\n",
    "if training_files:\n",
    "    sample_training_path = training_files[0]\n",
    "    with open(sample_training_path, 'r', encoding='utf-8') as f:\n",
    "        sample_training = f.read()\n",
    "    \n",
    "    print(f\"ðŸ“ Loaded training: {sample_training_path.name}\")\n",
    "    print(f\"Content length: {len(sample_training)} characters\")\n",
    "    print(f\"Estimated tokens: {estimate_tokens(sample_training)}\")\nelse:\n    print(\"No training files available for analysis\")\n    sample_training = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a simple classifier\ndef classify_seniority(content: str, model: str = \"mistral-small-latest\") -> str:\n    \"\"\"Figure out if this is entry-level, mid-level, or senior stuff\"\"\"\n    \n    prompt = f\"\"\"Look at this job/training content and tell me the seniority level.\n\nOptions:\n* Basic - Entry level, no experience needed\n* Intermediate - Some experience (1-3 years)\n* Advanced - Senior level (3+ years)\n\nJust respond with one word: Basic, Intermediate, or Advanced.\n\nContent:\n{content}\"\"\"\n    \n    result = call_mistral(prompt, model)\n    if result:\n        return result['content'].strip()\n    return \"Unknown\"\n\n# Test it on our sample\nif sample_training:\n    print(\"ðŸŽ¯ Testing the classifier...\")\n    classification = classify_seniority(sample_training)\n    print(f\"Result: {classification}\")\n    \n    print(\"\\nðŸ“‹ Here's what it analyzed:\")\n    display_markdown_file(sample_training_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing Strategy\n",
    "\n",
    "For efficiency, we should process multiple items at once when possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classify_trainings(training_files: list, batch_size: int = 5) -> dict:\n",
    "    \"\"\"Classify multiple trainings in batches to optimize API calls\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process first few files as example\n",
    "    sample_files = training_files[:3]  # Just process 3 for demo\n",
    "    \n",
    "    for i, file_path in enumerate(sample_files):\n",
    "        print(f\"Processing {i+1}/{len(sample_files)}: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            classification = classify_seniority(content)\n",
    "            results[file_path.name] = {\n",
    "                'seniority': classification,\n",
    "                'tokens': estimate_tokens(content)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "            results[file_path.name] = {'seniority': 'Error', 'tokens': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run batch classification\n",
    "if training_files:\n",
    "    print(\"ðŸ”„ Running batch classification (sample of 3 trainings)...\")\n",
    "    batch_results = batch_classify_trainings(training_files)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Results:\")\n",
    "    for filename, data in batch_results.items():\n",
    "        print(f\"{filename}: {data['seniority']} ({data['tokens']} tokens)\")\n",
    "        \n",
    "    # Calculate total tokens used\n",
    "    total_tokens = sum(data['tokens'] for data in batch_results.values())\n",
    "    print(f\"\\nðŸ’° Total tokens processed: {total_tokens}\")\n",
    "    # TODO: Add actual cost calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Caching - because we're not stupid\n\nProcess once, reuse forever. Basic optimization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_classification_cache(results: dict, filename: str = \"classification_cache.json\"):\n",
    "    \"\"\"Save classification results to avoid re-processing\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"âœ… Saved classification cache to {filename}\")\n",
    "\n",
    "def load_classification_cache(filename: str = \"classification_cache.json\") -> dict:\n",
    "    \"\"\"Load cached classification results\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"âœ… Loaded classification cache from {filename}\")\n",
    "        return results\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No cache file found: {filename}\")\n",
    "        return {}\n",
    "\n",
    "# Save our batch results\n",
    "if 'batch_results' in locals():\n",
    "    save_classification_cache(batch_results)\n",
    "\n",
    "# Demonstrate loading\n",
    "loaded_cache = load_classification_cache()\n",
    "print(f\"Cache contains {len(loaded_cache)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises (aka homework)\n\n### Exercise 1: Data analysis\nBuild some actual statistics about our dataset:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your mission: analyze the dataset properly\n# What we want to know:\n# - Jobs by domain (accounting, marketing, tourism, etc.)\n# - Geographic distribution (SÃ£o Paulo, Rio, etc.)\n# - Average token counts per category\n# - Most common skills mentioned\n\nprint(\"ðŸ“ Exercise 1: Data analysis\")\nprint(\"Use LLMs to extract domains from titles/content\")\nprint(\"Count location mentions\")  \nprint(\"Calculate processing costs for different approaches\")\n\n# Your code goes here...\n# Hint: Use the classify function pattern we just built"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Cost optimization\nFigure out the cheapest way to process everything:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cost comparison challenge\n# Calculate costs for:\n# 1. All 697 items with small model\n# 2. All 697 items with large model\n# 3. Hybrid: small for classification, large for complex analysis\n\nprint(\"ðŸ“ Exercise 2: Cost optimization\")\nprint(\"Which approach gives best quality/cost ratio?\")\nprint(\"What's the break-even point?\")\n\n# Your implementation here..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Green jobs detector\nBuild a classifier for sustainability-related jobs:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Green jobs classifier\ndef is_green_job(content: str) -> bool:\n    \"\"\"Detect sustainability/climate-related jobs and trainings\"\"\"\n    # Your implementation here\n    # Look for keywords like: renewable energy, sustainability, climate, environment\n    pass\n\nprint(\"ðŸ“ Exercise 3: Green jobs detector\")\nprint(\"Test it on the dataset - how many green opportunities can you find?\")\nprint(\"Bonus: what makes a job 'green'?\")\n\n# Implement the function and test it..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## What we learned\n\nâœ… **Data structure**: 697 items in messy formats  \nâœ… **API basics**: Tokens, models, costs  \nâœ… **Smart filtering**: LLMs > regex for unstructured data  \nâœ… **Cost optimization**: Start small, scale strategically  \n\n### The real lessons\n- Token counting matters when you're processing lots of data\n- Small models are surprisingly good for classification tasks\n- Caching is your friend\n- Always track costs as you go\n\n### Next up\nTutorial 3: Building your first submission and getting on the leaderboard ASAP.\n\n---\n\n## Notes for tutorial authors\n\n**TODOs that need real implementation:**\n- [ ] Get current Mistral pricing \n- [ ] Add actual token counting with Mistral tokenizer\n- [ ] Test all code cells with real data\n- [ ] Add Brazilian location context  \n- [ ] Implement the exercises properly\n\n**Stuff that works:**\n- Basic API setup and calls\n- Classification pattern\n- Cost tracking framework\n- Caching strategy\n\nStick with the conversational tone - this feels way more authentic than corporate training speak."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}