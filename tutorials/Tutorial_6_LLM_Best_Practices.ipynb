{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "681b305e-3bdf-48d9-afc8-68c1ba7fc345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: strands-agents-tools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.2.8)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: mistralai in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.9.10)\n",
      "Requirement already satisfied: tiktoken in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: strands-agents[mistral] in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.21)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.40.21)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (0.17.0)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.14.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (1.37.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (2.11.9)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents[mistral]) (6.0.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents[mistral]) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (2.5.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.10.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.4.1)\n",
      "Requirement already satisfied: httpx>=0.27.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.47.3)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.35.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (6.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents[mistral]) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.58b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (0.58b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.58b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (0.58b0)\n",
      "Requirement already satisfied: packaging>=18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.58b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents[mistral]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents[mistral]) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents[mistral]) (1.17.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (3.12.15)\n",
      "Requirement already satisfied: aws-requests-auth<0.5.0,>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (0.4.3)\n",
      "Requirement already satisfied: dill<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (0.4.0)\n",
      "Requirement already satisfied: markdownify<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (1.2.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (11.3.0)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.51 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (3.0.51)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (2.10.1)\n",
      "Requirement already satisfied: readabilipy<1.0.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (0.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (2.32.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (14.1.0)\n",
      "Requirement already satisfied: slack-bolt<2.0.0,>=1.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (1.25.0)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (1.14.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from strands-agents-tools) (9.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools) (1.20.1)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdownify<2.0.0,>=1.0.0->strands-agents-tools) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.9->markdownify<2.0.0,>=1.0.0->strands-agents-tools) (2.7)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools) (0.2.13)\n",
      "Requirement already satisfied: html5lib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from readabilipy<1.0.0,>=0.2.0->strands-agents-tools) (1.1)\n",
      "Requirement already satisfied: lxml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from readabilipy<1.0.0,>=0.2.0->strands-agents-tools) (6.0.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from readabilipy<1.0.0,>=0.2.0->strands-agents-tools) (2025.7.34)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.0->strands-agents-tools) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.0->strands-agents-tools) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.0->strands-agents-tools) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools) (2.19.2)\n",
      "Requirement already satisfied: slack_sdk<4,>=3.35.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools) (3.36.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools) (1.3.0)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai) (2.2.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mistralai) (6.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (0.27.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->strands-agents-tools) (0.1.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents[mistral]) (8.2.1)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from html5lib->readabilipy<1.0.0,>=0.2.0->strands-agents-tools) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv strands-agents[mistral] strands-agents-tools tqdm mistralai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "476c6828-0303-43d9-88b8-7903b7010372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai                               1.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e881c",
   "metadata": {},
   "source": [
    "# üß†üìâ Onboarding Notebook ‚Äî Eco- & Cost-Efficient Use of Mistral LLM via *La Plateforme* \n",
    "> Version generated on 2025-09-09\n",
    "\n",
    "\n",
    "Welcome! This notebook is part of the training material for your **data science challenge**.\n",
    "\n",
    "**Context:** You will build a solution that interacts with **100 personas** (simulated by LLMs) in natural language to guide their professional career. Each persona has an associated **gold file** describing the ground-truth recommendations your solution should steer them toward.\n",
    "\n",
    "**Goal:** Help you use Mistral LLM efficiently ‚Äî minimizing **cost**, **latency**, and **environmental impact** while keeping outputs reliable and reproducible for **testing vs. final submission**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424c0d2",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Prerequisites\n",
    "\n",
    "- **Account** and **API key** for Mistral (*La Plateforme*). Store it in the environment variable `MISTRAL_API_KEY`.\n",
    "- Python ‚â• 3.9, and (optionally) these packages:\n",
    "```bash\n",
    "pip install mistralai python-dotenv pandas matplotlib \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680a1d0",
   "metadata": {},
   "source": [
    "## 1) Setup: API key, client, quick verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "abd2658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Available models: ['mistral-medium-2505', 'mistral-large-latest', 'mistral-medium-2508', 'mistral-medium-latest', 'mistral-medium', 'ministral-3b-2410', 'ministral-3b-latest', 'ministral-8b-2410', 'ministral-8b-latest', 'open-mistral-7b', 'mistral-tiny', 'mistral-tiny-2312', 'open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-2407', 'mistral-tiny-latest', 'open-mixtral-8x7b', 'mistral-small', 'mistral-small-2312', 'open-mixtral-8x22b', 'open-mixtral-8x22b-2404', 'mistral-small-2409', 'mistral-large-2407', 'mistral-large-2411', 'pixtral-large-2411', 'pixtral-large-latest', 'mistral-large-pixtral-2411', 'codestral-2501', 'codestral-2412', 'codestral-2411-rc5', 'codestral-2508', 'codestral-latest', 'devstral-small-2505', 'devstral-small-2507', 'devstral-small-latest', 'devstral-medium-2507', 'devstral-medium-latest', 'pixtral-12b-2409', 'pixtral-12b', 'pixtral-12b-latest', 'mistral-small-2501', 'mistral-small-2503', 'mistral-small-2506', 'mistral-small-latest', 'mistral-saba-2502', 'mistral-saba-latest', 'magistral-medium-2506', 'magistral-medium-2507', 'magistral-small-2506', 'magistral-small-2507', 'magistral-medium-2509', 'magistral-medium-latest', 'magistral-small-2509', 'magistral-small-latest', 'voxtral-mini-2507', 'voxtral-mini-latest', 'voxtral-small-2507', 'voxtral-small-latest', 'mistral-embed-2312', 'mistral-embed', 'codestral-embed', 'codestral-embed-2505', 'mistral-moderation-2411', 'mistral-moderation-latest', 'mistral-ocr-2503', 'mistral-ocr-2505', 'mistral-ocr-latest', 'voxtral-mini-transcribe-2507', 'voxtral-mini-2507', 'voxtral-mini-latest']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../env\")\n",
    "API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"‚ö†Ô∏è Please set MISTRAL_API_KEY in your environment or a .env file.\")\n",
    "\n",
    "try:\n",
    "    from mistralai import Mistral\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install the SDK first: pip install mistralai\") from e\n",
    "\n",
    "client = Mistral(api_key=API_KEY)\n",
    "\n",
    "# Optional: list available models (fallback to a known one if unavailable)\n",
    "try:\n",
    "    available_models = [m.id for m in client.models.list().data]  # type: ignore\n",
    "    print(\"‚úÖ Available models:\", available_models)\n",
    "except Exception:\n",
    "    print(\"‚ÑπÔ∏è Could not list models; proceed with a known one (e.g., 'mistral-small-latest').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699a61b",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Efficiency Principles (cost, latency, energy)\n",
    "\n",
    "1. **Start small**: prototype with a *small* model first; scale only if needed.  \n",
    "2. **Strict bounds**: set conservative `max_tokens` and increase only with evidence.  \n",
    "3. **Concise prompts**: bullet points, explicit schemas, JSON output.  \n",
    "4. **Stop sequences**: stop generation when your structure is complete.  \n",
    "5. **Caching**: cache stable prompts and documents; hash inputs.  \n",
    "6. **Measure**: track tokens/latency/errors; **enforce budgets**.  \n",
    "7. **Determinism**: low temperature; seed if supported; avoid redundant retries.  \n",
    "8. **Clean inputs**: remove noise and unnecessary history.\n",
    "9. **Carbon aware scheduling**:Schedule heavy runs during low-carbon hours (e.g., nighttime or weekends in your region) if using cloud infrastructure. This can reduce environmental impact when used at scale.\n",
    "10. **Submission fingerprinting**:Include model version, temperature, token usage, and prompt hash. This helps with reproducibility and auditability without storing full logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719dd32",
   "metadata": {},
   "source": [
    "## 3) Minimal call with strict token control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fffcee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting max_tokens prevents excessively long responses, saving computational resources and time. It also helps maintain context relevance and avoids unnecessary verbosity. However, setting it too low may truncate important information. Balance is key.\n",
      "usage: prompt_tokens=38 completion_tokens=44 total_tokens=82 prompt_audio_seconds=Unset()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This snippet shows how to make a safe, efficient, and reproducible call to the Mistral LLM using a structured prompt and \n",
    "controlled generation settings.\n",
    "\n",
    "What the Code Does:\n",
    "Defines a system prompt that sets clear behavior: concise answers under 60 words, and honesty when uncertain.\n",
    "\n",
    "Sends a user query asking why limiting max_tokens is important.\n",
    "\n",
    "Calls the Mistral model with:\n",
    "\n",
    "max_tokens=120: limits the length of the response to control cost and latency.\n",
    "temperature=0.2: encourages deterministic, focused output.\n",
    "top_p=0.9: controls diversity of output (optional).\n",
    "(Optional) stop and response_format parameters for cleaner output or structured JSON.\n",
    "Prints the model‚Äôs reply and usage stats:\n",
    "\n",
    "prompt_tokens: tokens used in the input\n",
    "completion_tokens: tokens used in the output\n",
    "total_tokens: total cost-relevant token count\n",
    "\"\"\"\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "MODEL_SMALL = \"mistral-small-latest\"  # adjust to your org's models\n",
    "\n",
    "system = \"You are a concise assistant. Answer in <60 words. If unsure, say so.\"\n",
    "user = \"Explain why limiting max_tokens is important when calling an LLM.\"\n",
    "\n",
    "resp = client.chat.complete(\n",
    "    model=MODEL_SMALL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ],\n",
    "    max_tokens=120,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    # stop=[\"\\nEND\"]  # add if you know your stopping delimiter\n",
    "    # response_format={\"type\":\"json_object\"}  # add if you want JSON\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)   # type: ignore\n",
    "print(\"usage:\", resp.usage)  # prompt_tokens, completion_tokens, total_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9f0cf",
   "metadata": {},
   "source": [
    "## 4) Compact JSON output (less verbosity, easier post-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25724233-8c97-4349-9ac4-cbe73e33e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code demonstrates how to request structured output from the Mistral LLM using the response_format={\"type\": \"json_object\"} parameter.\n",
    "\n",
    "What the Code Does:\n",
    "Defines a prompt asking the model to give 3 reasons to reduce token usage, and to reply in a specific JSON format.\n",
    "Sends the prompt to the Mistral model (MODEL_SMALL) with:\n",
    "max_tokens=120: limits the response length.\n",
    "temperature=0.2: ensures low variability for reproducibility.\n",
    "response_format={\"type\": \"json_object\"}: instructs the model to return a valid JSON object.\n",
    "Prints the model‚Äôs response and the token usage metadata.\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the cache directory\n",
    "CACHE_DIR = \"../mistral_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Generate a unique cache key based on model, messages, and other parameters\n",
    "def _cache_key(model: str, messages: list, **kwargs) -> str:\n",
    "    blob = json.dumps({\"model\": model, \"messages\": messages, **kwargs}, sort_keys=True).encode()\n",
    "    return hashlib.sha256(blob).hexdigest()\n",
    "\n",
    "# Main function to handle caching logic\n",
    "def chat_with_cache(model: str, messages: list, **kwargs):\n",
    "    print(\"Start\")\n",
    "    key = _cache_key(model, messages, **kwargs)\n",
    "    path = os.path.join(CACHE_DIR, key)\n",
    "\n",
    "    # Check if response is already cached\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data[\"from_cache\"] = True\n",
    "        return data\n",
    "\n",
    "    # If not cached, call the Mistral API\n",
    "    out = client.chat.complete(model=model, messages=messages, **kwargs)\n",
    "    data = {\n",
    "        \"content\": out.choices[0].message.content,\n",
    "        \"usage\": getattr(out, \"usage\", None),\n",
    "        \"created\": int(time.time())\n",
    "    }\n",
    "\n",
    "    # Convert usage object to dictionary if needed\n",
    "    if data[\"usage\"] and hasattr(data[\"usage\"], '__dict__'):\n",
    "        data[\"usage\"] = data[\"usage\"].__dict__\n",
    "\n",
    "    # Save to cache\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    data[\"from_cache\"] = False\n",
    "    return data\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76203d27",
   "metadata": {},
   "source": [
    "## 5) Simple local cache (avoid unnecessary model calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2b62ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This utility caches responses from a language model (LLM) locally to avoid repeated API calls during development and testing. \n",
    "It helps reduce latency, cost, and environmental impact while ensuring consistent behavior across identical inputs.\n",
    "\n",
    "What the Code Does:\n",
    "- Creates a local cache directory (`.mistral_cache`) to store model responses.\n",
    "- Generates a unique cache key using a SHA-256 hash of:\n",
    "  ‚Ä¢ Model name\n",
    "  ‚Ä¢ Message history\n",
    "  ‚Ä¢ Additional parameters (e.g., temperature, max_tokens)\n",
    "- Checks if a cached response exists:\n",
    "  ‚Ä¢ If found, loads it from disk and marks it as `\"from_cache\": True`\n",
    "  ‚Ä¢ If not found, calls the Mistral API, saves the response, and marks it as `\"from_cache\": False`\n",
    "- Returns the response content along with usage metadata (e.g., token counts)\n",
    "\n",
    "This approach is especially useful during iterative prompt tuning, model evaluation, or experimentation in data science workflows.\n",
    "\"\"\"\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "CACHE_DIR = \".mistral_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "def _cache_key(model: str, messages: list, **kwargs) -> str:\n",
    "    blob = json.dumps({\"model\": model, \"messages\": messages, **kwargs}, sort_keys=True).encode()\n",
    "    return hashlib.sha256(blob).hexdigest()\n",
    "def chat_with_cache(model: str, messages: list, **kwargs):\n",
    "    key = _cache_key(model, messages, **kwargs)\n",
    "    path = os.path.join(CACHE_DIR, key + \".json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data[\"from_cache\"] = True\n",
    "        return data\n",
    "    out = client.chat.complete(model=model, messages=messages, **kwargs)\n",
    "    # Safely serialize usage\n",
    "    usage = getattr(out, \"usage\", None)\n",
    "    if usage:\n",
    "        try:\n",
    "            usage = json.loads(json.dumps(usage, default=lambda o: o.__dict__))\n",
    "        except Exception:\n",
    "            usage = str(usage)\n",
    "    data = {\n",
    "        \"content\": out.choices[0].message.content,\n",
    "        \"usage\": usage,\n",
    "        \"created\": int(time.time()),\n",
    "        \"from_cache\": False\n",
    "    }\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2af4c",
   "metadata": {},
   "source": [
    "## 6) Track usage and enforce budgets (lightweight logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e587c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Log file: /home/ec2-user/SageMaker/conversations_tej/mistral_usage_log.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code helps you track and log the usage of a language model (LLM) like Mistral during your experiments. \n",
    "It‚Äôs especially useful when you're testing prompts, measuring latency, or analyzing token consumption.\n",
    "\n",
    "What Does This Code Do?\n",
    "Defines a structure to store usage data\n",
    "Logs each LLM call to a CSV file\n",
    "Wraps the LLM call in a safe function that handles errors and logs performance\n",
    "\"\"\"\n",
    "\n",
    "import csv, time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "LOG_PATH = Path(\"mistral_usage_log.csv\")\n",
    "\n",
    "@dataclass\n",
    "class UsageRecord:\n",
    "    ts: float\n",
    "    model: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_s: float\n",
    "    ok: bool\n",
    "\n",
    "def log_usage(rec: UsageRecord):\n",
    "    exists = LOG_PATH.exists()\n",
    "    with open(LOG_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow([\"ts\",\"model\",\"prompt_tokens\",\"completion_tokens\",\"total_tokens\",\"latency_s\",\"ok\"])\n",
    "        w.writerow([rec.ts, rec.model, rec.prompt_tokens, rec.completion_tokens, rec.total_tokens, rec.latency_s, rec.ok])\n",
    "\n",
    "def safe_chat(model: str, messages: list, **kwargs) -> str:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        out = client.chat.complete(model=model, messages=messages, **kwargs)\n",
    "        latency = time.time() - t0\n",
    "        # Safely convert usage to dict\n",
    "        usage = getattr(out, \"usage\", None)\n",
    "        if usage:\n",
    "            try:\n",
    "                usage = json.loads(json.dumps(usage, default=lambda o: o.__dict__))\n",
    "            except Exception:\n",
    "                usage = {}\n",
    "\n",
    "        log_usage(UsageRecord(\n",
    "            ts=time.time(),\n",
    "            model=model,\n",
    "            prompt_tokens=usage.get(\"prompt_tokens\", 0),\n",
    "            completion_tokens=usage.get(\"completion_tokens\", 0),\n",
    "            total_tokens=usage.get(\"total_tokens\", 0),\n",
    "            latency_s=latency,\n",
    "            ok=True\n",
    "        ))\n",
    "\n",
    "        return out.choices[0].message.content  # type: ignore\n",
    "\n",
    "    except Exception as e:\n",
    "        latency = time.time() - t0\n",
    "        log_usage(UsageRecord(\n",
    "            ts=time.time(),\n",
    "            model=model,\n",
    "            prompt_tokens=0,\n",
    "            completion_tokens=0,\n",
    "            total_tokens=0,\n",
    "            latency_s=latency,\n",
    "            ok=False\n",
    "        ))\n",
    "        raise\n",
    "\n",
    "# Example\n",
    "txt = safe_chat(\n",
    "    MODEL_SMALL,\n",
    "    [{\"role\":\"user\", \"content\":\"Reply with just 'ok'\"}],\n",
    "    max_tokens=3, temperature=0\n",
    ")\n",
    "print(txt)\n",
    "print(f\"Log file: {LOG_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c9566",
   "metadata": {},
   "source": [
    "### 7)Visualize usage quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "34df55bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.758186e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272914</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.758186e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.223315</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.758187e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.212862</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.758197e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.212736</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.758274e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.296407</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ts                 model  prompt_tokens  completion_tokens  \\\n",
       "1  1.758186e+09  mistral-small-latest              0                  0   \n",
       "2  1.758186e+09  mistral-small-latest              9                  2   \n",
       "3  1.758187e+09  mistral-small-latest              9                  2   \n",
       "4  1.758197e+09  mistral-small-latest              9                  2   \n",
       "5  1.758274e+09  mistral-small-latest              9                  2   \n",
       "\n",
       "   total_tokens  latency_s     ok  \n",
       "1             0   0.272914  False  \n",
       "2            11   0.223315   True  \n",
       "3            11   0.212862   True  \n",
       "4            11   0.212736   True  \n",
       "5            11   0.296407   True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code helps analyze your LLM usage over time by reading the mistral_usage_log.csv file and plotting how many tokens were used in each call.\n",
    "\n",
    "What the Code Does:\n",
    "Checks if the usage log file exists (mistral_usage_log.csv).\n",
    "If it exists:\n",
    "Loads the log into a pandas DataFrame.\n",
    "Displays the last 5 entries to give a quick snapshot of recent usage.\n",
    "Plots the total_tokens column to show how token usage varies across calls.\n",
    "If the file doesn‚Äôt exist:\n",
    "Prints a message saying no logs are available yet.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if LOG_PATH.exists():\n",
    "    df = pd.read_csv(LOG_PATH)\n",
    "    display(df.tail(5))\n",
    "    plt.figure()\n",
    "    df['total_tokens'].plot(title=\"Total tokens per call\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No logs yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0f78e",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Choosing the right model\n",
    "\n",
    "- **Heuristic**: *small* for extraction/formatting/JSON, *medium* for moderate reasoning, *large* only if strictly necessary.  \n",
    "- **Local benchmark**: create a small eval set (20‚Äì50 cases); compare **quality vs. cost/latency**.  \n",
    "- **Solution profiles**: define presets (`FAST_CHEAP`, `BALANCED`, `SMART`) and **default to the frugal one**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42039ad7",
   "metadata": {},
   "source": [
    "## 9) Robustness: timeouts, retries, exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "56913cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code defines a retry mechanism that wraps your Mistral API call to make it more robust and fault-tolerant.\n",
    "If the call fails due to a transient error (like a network glitch or rate limit), \n",
    "it will automatically retry up to 3 times using exponential backoff with jitter‚Äîa technique that increases the wait time \n",
    "between retries while adding randomness to avoid overloading the server.\"\"\"\n",
    "\n",
    "import random, time\n",
    "\n",
    "def retry(fn, attempts=3, base=0.5, jitter=0.3):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception:\n",
    "            if i == attempts - 1:\n",
    "                raise\n",
    "            delay = base * (2**i) + random.uniform(0, jitter)\n",
    "            time.sleep(delay)\n",
    "\n",
    "res = retry(lambda: client.chat.complete(\n",
    "    model=MODEL_SMALL,\n",
    "    messages=[{\"role\":\"user\", \"content\":\"Reply 'ok'\"}],\n",
    "    max_tokens=3, temperature=0\n",
    "))\n",
    "print(res.choices[0].message.content)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5018e",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Governance, Security & Privacy\n",
    "\n",
    "- **Log only essentials** (hash inputs rather than storing raw text if feasible).  \n",
    "- **Guardrails**: input size limits, content filters, output JSON schema validation.  \n",
    "- **Version prompts & configs** (git) and link results to artifact hashes.  \n",
    "- **Separate environments** (local/submission) and API keys with roles/quotas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e343345",
   "metadata": {},
   "source": [
    "\n",
    "## 11) (Optional) Rough environmental impact estimation\n",
    "\n",
    "> Exact footprint of API calls is not directly observable. You can still track tokens and apply an **internal factor** (gCO‚ÇÇe per 1k tokens) for relative comparisons.\n",
    "\n",
    "Formula: `impact ‚âà total_tokens √ó factor_gco2_per_1k`. Use for **relative** comparisons only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d5789d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>est_gco2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.758185e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.758186e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.758186e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>11</td>\n",
       "      <td>0.03135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.758187e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>11</td>\n",
       "      <td>0.03135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.758197e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>11</td>\n",
       "      <td>0.03135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.758274e+09</td>\n",
       "      <td>mistral-small-latest</td>\n",
       "      <td>11</td>\n",
       "      <td>0.03135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ts                 model  total_tokens  est_gco2\n",
       "0  1.758185e+09  mistral-small-latest             0   0.00000\n",
       "1  1.758186e+09  mistral-small-latest             0   0.00000\n",
       "2  1.758186e+09  mistral-small-latest            11   0.03135\n",
       "3  1.758187e+09  mistral-small-latest            11   0.03135\n",
       "4  1.758197e+09  mistral-small-latest            11   0.03135\n",
       "5  1.758274e+09  mistral-small-latest            11   0.03135"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total estimate (gCO2e): 0.13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This script helps track and estimate the environmental impact of Mistral LLM usage by calculating the carbon footprint based on total tokens used.\n",
    "It reads mistral usage log that contains token counts per run.It applies a simple formula to estimate emissions. \n",
    "It then adds a new column est_gco2 to the log and displays the last 10 entries.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "G_CO2_PER_1K_TOKENS = 2.85   #Grams of carbon dioxide equivalent per 1000 tokens\n",
    "\n",
    "def estimate_emissions_gco2(total_tokens:int, factor_per_1k:float=G_CO2_PER_1K_TOKENS)->float:\n",
    "    return (total_tokens / 1000.0) * factor_per_1k\n",
    "\n",
    "from pathlib import Path\n",
    "LOG_PATH = Path(\"mistral_usage_log.csv\")\n",
    "\n",
    "if LOG_PATH.exists():\n",
    "    df = pd.read_csv(LOG_PATH)\n",
    "    df[\"est_gco2\"] = df[\"total_tokens\"].apply(estimate_emissions_gco2)\n",
    "    display(df[[\"ts\",\"model\",\"total_tokens\",\"est_gco2\"]].tail(10))\n",
    "    print(f\"Total estimate (gCO2e): {df['est_gco2'].sum():.2f}\")\n",
    "else:\n",
    "    print(\"No usage log yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab10de",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Frugality checklist **before final submission**\n",
    "\n",
    "- [ ] **Smallest sufficient model** is default.  \n",
    "- [ ] `max_tokens` and **stop** set.  \n",
    "- [ ] **JSON output** where relevant; **compact prompts**.  \n",
    "- [ ] **Cache** used in local testing; **disabled for submission** (no stale answers).  \n",
    "- [ ] **Budgets** in place (tokens/calls/day).  \n",
    "- [ ] **Usage logging** (tokens, latency, errors) + minimal dashboard.  \n",
    "- [ ] **Eval set** (20‚Äì50 cases) shows quality/cost/latency trade-offs.  \n",
    "- [ ] **Privacy**: minimization, guardrails, lean logs.  \n",
    "- [ ] **Retries/backoff** + **timeouts** configured.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07ec86",
   "metadata": {},
   "source": [
    "## 13) Assistant specifics: conversational memory & rolling summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9b000b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Token counting & rolling summary utilities\n",
    "\"\"\"This utility helps track token usage and manage prompt length efficiently when working with Mistral LLM.\n",
    "\n",
    "What it does:\n",
    "Estimates token count using tiktoken to approximate how many tokens a message will consume.\n",
    "Trims message history to stay within a token budget, keeping only the most relevant parts (e.g., system prompt + last user turns).\n",
    "Maintains a rolling summary of the conversation using the model itself, summarizing recent turns into a compact bullet list. \n",
    "This helps reduce prompt size while preserving context.\"\"\"\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "from typing import List\n",
    "\n",
    "ENC = tiktoken.get_encoding(\"cl100k_base\")  # approx for counting\n",
    "\n",
    "def approx_tokens(text: str) -> int:\n",
    "    return len(ENC.encode(text))\n",
    "\n",
    "def total_tokens_messages(msgs: List[dict]) -> int:\n",
    "    return sum(approx_tokens(getattr(m, \"content\", \"\")) for m in msgs)\n",
    "\n",
    "def trim_messages(msgs: List[dict], max_prompt_tokens: int) -> List[dict]:\n",
    "    system = [m for m in msgs if m.role == \"system\"]\n",
    "    others = [m for m in msgs if m.role != \"system\"]\n",
    "    base = (system[:1] + others[-2:]) if len(others) >= 2 else msgs[:]\n",
    "    if total_tokens_messages(base) <= max_prompt_tokens:\n",
    "        return base\n",
    "    return (system[:1] + [m for m in others if m.role == \"user\"][-1:])\n",
    "\n",
    "ROLLING_SUMMARY = \"\"\n",
    "\n",
    "def update_rolling_summary(client, model: str, msgs: List[dict]) -> str:\n",
    "    global ROLLING_SUMMARY\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"Summarize into ‚â§5 factual bullets, in English, no verbosity.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Current summary:\\n{ROLLING_SUMMARY}\\n\\nNew turns:\\n{[(m['role'], m['content']) for m in msgs[-3:]]}\\n\\nUpdate the summary.\"}\n",
    "    ]\n",
    "    out = client.chat.complete(model=model, messages=prompt, max_tokens=120, temperature=0.1)\n",
    "    ROLLING_SUMMARY = out.choices[0].message.content  # type: ignore\n",
    "    return ROLLING_SUMMARY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61939dd8",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Minimal system prompt for assistants\n",
    "\n",
    "Keep the **contract short** to save tokens each turn:\n",
    "\n",
    "> *\"You are a concise English assistant. Answer in ‚â§120 words. If a table is requested, return valid JSON `{columns:[], rows:[]}`. For code, return code only. If unsure, say so and ask one clarifying question. Use only the provided context.\"*\n",
    "Replace verbose instructions with symbolic or shorthand cues (e.g., #plan, #tool_call) that are interpreted by the model but save tokens. This is especially useful in multi-step agentic flows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11aebc",
   "metadata": {},
   "source": [
    "## 15) Function-calling (if supported): minimal schemas & stop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2551dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'get_excel_content',\n",
       "   'description': 'Reads and returns content from an Excel file. You can specify sheet name and optionally a cell range.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'file_path': {'type': 'string',\n",
       "      'description': 'Path to the Excel file.'},\n",
       "     'sheet_name': {'type': 'string',\n",
       "      'description': 'Name of the sheet to read from.'},\n",
       "     'range': {'type': 'string',\n",
       "      'description': \"Optional cell range (e.g., 'A1:C10').\"}},\n",
       "    'required': ['file_path', 'sheet_name'],\n",
       "    'additionalProperties': 'false'}}}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This utility defines a tool specification for reading Excel file content via a structured function call, \n",
    "enabling integration with chat-based AI systems.\n",
    "What it does:\n",
    "*Declares a function named get_excel_content for use in tool-based AI workflows.\n",
    "*Enables reading data from an Excel file by specifying:\n",
    "  -File path\n",
    "  -Sheet name\n",
    "  -Optional cell range (e.g., \"A1:C10\")\n",
    "*Supports automatic tool selection (tool_choice=\"auto\") for intelligent invocation.\n",
    "*Ensures controlled output via max_tokens, temperature, and stop conditions.\"\"\"\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_excel_content\",\n",
    "      \"description\": \"Reads and returns content from an Excel file. You can specify sheet name and optionally a cell range.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"file_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the Excel file.\"\n",
    "          },\n",
    "          \"sheet_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the sheet to read from.\"\n",
    "          },\n",
    "          \"range\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Optional cell range (e.g., 'A1:C10').\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"file_path\", \"sheet_name\"],\n",
    "        \"additionalProperties\": \"false\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "# Example call (adapt to your SDK version):\n",
    "# resp = client.chat.complete(\n",
    "#   model=MODEL_SMALL,\n",
    "#   messages=[...],\n",
    "#   tools=TOOLS_SPEC,\n",
    "#   tool_choice=\"auto\",\n",
    "#   max_tokens=160,\n",
    "#   temperature=0.2,\n",
    "#   stop=[\"</tool_output>\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a64f13",
   "metadata": {},
   "source": [
    "## 16) Agent observability: lean logs, metrics, and circuit breakers with submission fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6fb7ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This utility logs each agent run to a CSV file (agent_runs.csv) for traceability, debugging, and performance analysis.\n",
    "\n",
    "What it does:\n",
    "Records each tool invocation made during an agent run.\n",
    "Captures the following details for each step:\n",
    "Timestamp (in human-readable ISO format)\n",
    "Run ID\n",
    "Step number\n",
    "Tool name\n",
    "Latency (in seconds)\n",
    "Total tokens used for the run\n",
    "Automatically creates the CSV file with headers if it doesn't exist.\n",
    "Uses structured logging via csv.DictWriter for clarity and consistency.\n",
    "Includes error handling to ensure robustness during file operations.\"\"\"\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "AGENT_LOG = Path(\"agent_runs.csv\")\n",
    "\n",
    "def log_agent_run(run_id: str, trace: list, used_tokens: int):\n",
    "    fieldnames = [\"timestamp\", \"run_id\", \"step\", \"tool\", \"latency_s\", \"used_tokens\"]\n",
    "    log_exists = AGENT_LOG.exists()\n",
    "\n",
    "    try:\n",
    "        with open(AGENT_LOG, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if not log_exists:\n",
    "                writer.writeheader()\n",
    "            for t in trace:\n",
    "                writer.writerow({\n",
    "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                    \"run_id\": run_id,\n",
    "                    \"step\": t.get(\"step\"),\n",
    "                    \"tool\": t.get(\"tool\", \"\"),\n",
    "                    \"latency_s\": t.get(\"latency_s\", 0.0),\n",
    "                    \"used_tokens\": used_tokens\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging agent run: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1ecd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This code helps you track each agent run and generate a minimal fingerprint for your final submission. \n",
    "\n",
    "What the Code Does:\n",
    "Defines a log file (agent_runs.csv) to store metadata about each run.\n",
    "Generates a SHA256 hash of the prompt using hashlib, so you don‚Äôt need to store raw prompt text.\n",
    "Logs each run with:\n",
    "Timestamp (ts)\n",
    "Run ID (run_id)\n",
    "Model name (model)\n",
    "Temperature (temperature)\n",
    "Total tokens used (total_tokens)\n",
    "Prompt hash (prompt_hash)\n",
    "Prints a fingerprint in JSON format that you can include with your final submission.\n",
    "\"\"\"\n",
    "\n",
    "import csv, time, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_SMALL = \"mistral-small-latest\"\n",
    "AGENT_LOG = Path(\"agent_runs.csv\")\n",
    "\n",
    "def hash_prompt(messages):\n",
    "    full_prompt = \"\\n\".join(f\"{m.role}: {m.content}\" for m in messages)\n",
    "    return hashlib.sha256(full_prompt.encode()).hexdigest()\n",
    "\n",
    "def log_agent_run(run_id: str, messages: list, used_tokens: int, temperature: float):\n",
    "    exists = AGENT_LOG.exists()\n",
    "    prompt_hash = hash_prompt(messages)\n",
    "    timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "\n",
    "    with open(AGENT_LOG, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow([\"ts\", \"run_id\", \"model\", \"temperature\", \"total_tokens\", \"prompt_hash\"])\n",
    "        w.writerow([timestamp, run_id, MODEL_SMALL, temperature, used_tokens, prompt_hash])\n",
    "\n",
    "    # Optional: print fingerprint for submission\n",
    "    fingerprint = {\n",
    "        \"model\": MODEL_SMALL,\n",
    "        \"temperature\": temperature,\n",
    "        \"total_tokens_used\": used_tokens,\n",
    "        \"prompt_hash\": prompt_hash\n",
    "    }\n",
    "    print(\"Submission Fingerprint:\")\n",
    "    print(json.dumps(fingerprint, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e058c46",
   "metadata": {},
   "source": [
    "\n",
    "## 17) Challenge specifics: **testing vs submission** with personas & gold files\n",
    "\n",
    "- **Personas**: 100 LLM-driven personas to interact with in natural language.  \n",
    "- **Gold files**: each persona has a ground-truth recommendation set your solution should surface.  \n",
    "- **Testing** (local): smaller models, aggressive caching, verbose logs, sandboxed tools.  \n",
    "- **Submission** (final run): deterministic settings (low temperature), **no cache reuse**, strict budgets, concise prompts, validated JSON outputs, lean logs.  \n",
    "- **Evaluation tip**: build a small **offline eval set** mapping persona snippets ‚Üí expected gold recommendations, measure accuracy + tokens/latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c39e28a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "üéØ **Key takeaway**: *Start small, measure everything, enforce limits, and design for frugality from the start.*  \n",
    "Good luck with the challenge!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b842b-8fc4-4f5b-8498-88a5ff6830a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c023-a728-4a7c-af04-889d6aa357ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
