{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Optimized Multi-Agent System\n",
    "\n",
    "You crushed Tutorial 4! Your agents are working, you're seeing ~30% accuracy, and you understand the multi-agent architecture. But the leaderboard leaders are hitting 60-70%. Time to close that gap.\n",
    "\n",
    "## What's New in Tutorial 5\n",
    "\n",
    "Tutorial 4 was about **building** a working system. Tutorial 5 is about **optimizing** it for production-level performance.\n",
    "\n",
    "### Key Improvements We're Adding:\n",
    "\n",
    "**Work Type Pre-Filtering**\n",
    "- Focus on remote vs onsite work preferences\n",
    "- Filter jobs by location AND work type before LLM matching\n",
    "\n",
    "**Token Optimization Everywhere**\n",
    "- Smarter extraction prompts that get work type preferences in one pass\n",
    "- Pre-filtering reduces LLM context from all jobs to only relevant ones\n",
    "\n",
    "**Precision Matching**\n",
    "- Work type alignment prevents mismatches (no remote workers getting onsite-only jobs)\n",
    "- Location+work type filtering creates much better candidate pools\n",
    "\n",
    "## The Math That Matters\n",
    "\n",
    "**Tutorial 4 approach**:\n",
    "- Send ALL jobs to LLM for every persona\n",
    "- Large token count per matching request\n",
    "- Process all available jobs regardless of relevance\n",
    "\n",
    "**Tutorial 5 approach**:\n",
    "- Pre-filter to relevant jobs per persona based on work type preference\n",
    "- Reduced token count per matching request\n",
    "- Process only jobs that match work type preferences\n",
    "\n",
    "## Architecture Improvements\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  OPTIMIZED MULTI-AGENT ARCHITECTURE             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Phase 1: ENHANCED INTERVIEW & EXTRACTION\n",
    "=========================================\n",
    "  Persona API\n",
    "      ‚Üì\n",
    "  [Interview Agent] ‚Üê Now asks about work type preferences\n",
    "      ‚Üì\n",
    "  Raw Transcript\n",
    "      ‚Üì\n",
    "  [Persona Extraction Agent] ‚Üê Extracts work type info\n",
    "      ‚Üì\n",
    "  PersonaInfo (with work type)\n",
    "\n",
    "Phase 2: WORK TYPE-AWARE DATA PROCESSING  \n",
    "========================================\n",
    "  Job Files ‚Üí [Job Extraction Agent] ‚Üí JobInfo (with work types)\n",
    "  Training Files ‚Üí [Training Extraction Agent] ‚Üí TrainingInfo\n",
    "\n",
    "Phase 3: PRE-FILTERED INTELLIGENT MATCHING\n",
    "===========================================\n",
    "  PersonaInfo + Work Type Filter\n",
    "            ‚Üì\n",
    "       [Pre-Filter] ‚Üê Reduces job pool to relevant work types\n",
    "            ‚Üì\n",
    "    Filtered JobInfo + PersonaInfo\n",
    "            ‚Üì\n",
    "      [Matching Agent] ‚Üê Works with relevant subset only\n",
    "            ‚Üì\n",
    "      Better Recommendations\n",
    "```\n",
    "\n",
    "**The secret sauce**: We don't change the agent logic, we just give each agent better, more focused data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import dotenv\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypeVar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to import our utilities\n",
    "sys.path.append('..')\n",
    "from src.utils import (\n",
    "    save_json,\n",
    "    read_json,\n",
    "    load_file_content,\n",
    "    get_job_paths,\n",
    "    get_training_paths,\n",
    "    sanity_check,\n",
    "    track_api_call,  # Cost tracking from utils\n",
    "    print_cost_summary,  # Cost summary from utils\n",
    "    reset_cost_tracker  # Reset cost tracker from utils\n",
    ")\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "# Strands for AI agents\n",
    "from strands.agent import Agent\n",
    "from strands.models.mistral import MistralModel\n",
    "\n",
    "# AWS authentication\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "# Type hints\n",
    "M = TypeVar('M', bound=BaseModel)\n",
    "\n",
    "# Set up submission directory\n",
    "SUBMISSION_DIR = Path('../submissions/tutorial_5')\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load environment\n",
    "dotenv.load_dotenv(\"../env\")\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Data Models (The Key Optimization)\n",
    "\n",
    "Here's the game-changer for Tutorial 5: **Work type-based filtering**. \n",
    "\n",
    "**Tutorial 4 problem**: We sent ALL jobs to the LLM for every persona, even when many were irrelevant based on work preferences.\n",
    "- Remote workers see onsite-only jobs ‚Üí wastes tokens, confuses matching\n",
    "- Onsite workers see remote-only jobs ‚Üí wastes tokens, confuses matching\n",
    "\n",
    "**Tutorial 5 solution**: Add `work_type` field to both personas and jobs. Pre-filter before LLM matching.\n",
    "\n",
    "### The Two Work Types\n",
    "\n",
    "Based on analysis of the job dataset and persona preferences, we focus on two distinct work arrangements:\n",
    "1. **`remote`**: Work from home, distributed teams, digital collaboration\n",
    "2. **`onsite`**: Office-based, in-person collaboration, physical presence required\n",
    "\n",
    "### Why Work Type Filtering Works\n",
    "\n",
    "**Semantic matching without work type filtering**:\n",
    "```\n",
    "Maria (remote preference) ‚Üí Gets matched with:\n",
    "- Remote Software Developer (perfect!) \n",
    "- Onsite Marketing Manager (wrong - she wants remote)\n",
    "- Hybrid Project Manager (maybe - depends on flexibility)\n",
    "- Onsite Sales Representative (wrong - not remote)\n",
    "```\n",
    "\n",
    "**Work type-aware matching**:\n",
    "```\n",
    "Maria (remote preference) ‚Üí Pre-filtered to remote jobs only:\n",
    "- Remote Software Developer (perfect!)\n",
    "- Remote Content Writer (great match!)\n",
    "- Remote Data Analyst (excellent!)\n",
    "- Remote Customer Success (ideal!)\n",
    "```\n",
    "\n",
    "**Result**: LLM sees only relevant jobs ‚Üí better matches + reduced tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Enhanced structured profile of a job seeker with category preference\n",
    "    \"\"\"\n",
    "    name: str = Field(default=\"\", description=\"Persona's name\")\n",
    "    skills: List[Tuple[str, str]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of (skill, level) pairs\"\n",
    "    )\n",
    "    location: str = Field(default=\"unknown\")\n",
    "    age: str = Field(default=\"unknown\")\n",
    "    years_of_experience: str = Field(default=\"unknown\")\n",
    "    work_type: str = Field(default=\"unspecified\", description=\"Preffered work type - one of two: 'onsite' or 'remote'\")\n",
    "\n",
    "    def describe(self) -> str:\n",
    "        \"\"\"Enhanced description that includes category for better matching\"\"\"\n",
    "        skills = ', '.join([f'{s}: {l}' for s, l in self.skills])\n",
    "        return f\"Name: {self.name}\\nSkills: {skills}\\nLocation: {self.location}\\nAge: {self.age}\\nExperience: {self.years_of_experience} years\\nWork type: {self.work_type}\"\n",
    "\n",
    "\n",
    "class JobInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Enhanced structured job requirements with category classification\n",
    "    \"\"\"\n",
    "    required_skills: List[str] = Field(default_factory=list)\n",
    "    location: str = Field(default=\"\")\n",
    "    years_of_experience_required: str = Field(default=\"\")\n",
    "    work_type: str = Field(default=\"unspecified\", description=\"One of two: 'onsite' or 'remote'\")\n",
    "\n",
    "    def describe(self) -> str:\n",
    "        \"\"\"Enhanced description that includes category for filtering\"\"\"\n",
    "        skills = ', '.join(self.required_skills)\n",
    "        return f\"Skills: {skills}\\nLocation: {self.location}\\nExperience: {self.years_of_experience_required}\\nWork type: {self.work_type}\"\n",
    "\n",
    "\n",
    "class TrainingInfo(BaseModel):\n",
    "    \"\"\"Training program details - unchanged from Tutorial 4\n",
    "    \n",
    "    Note: We kept this simple since training matching is less critical\n",
    "    and the cost savings from job pre-filtering are already substantial.\n",
    "    \"\"\"\n",
    "    skill_acquired_and_level: Tuple[str, str] = Field(\n",
    "        default=(\"not specified\", \"not specified\")\n",
    "    )\n",
    "\n",
    "    def describe(self) -> str:\n",
    "        skill, level = self.skill_acquired_and_level\n",
    "        return f\"Teaches: {skill} (Level: {level})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Agent Factory (Same as Tutorial 4)\n",
    "\n",
    "No changes here - our agent factory is already optimized. The improvements in Tutorial 5 come from:\n",
    "1. **Better data** (category fields)\n",
    "2. **Smarter prompts** (asking for categories)  \n",
    "3. **Pre-filtering** (reducing tokens before LLM calls)\n",
    "\n",
    "The agent architecture itself is solid. We're optimizing the data flow, not rebuilding the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(\n",
    "    system_prompt: str = \"\",\n",
    "    model_id: str = \"mistral-medium-latest\"\n",
    ") -> Agent:\n",
    "    \"\"\"Create an AI agent with specific role and model\"\"\"\n",
    "    model = MistralModel(\n",
    "        api_key=os.environ[\"MISTRAL_API_KEY\"],\n",
    "        model_id=model_id,\n",
    "        stream=False\n",
    "    )\n",
    "    return Agent(model=model, system_prompt=system_prompt, callback_handler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 1: Enhanced Interview Agent\n",
    "\n",
    "- **Purpose**: Conduct structured interviews with personas (now includes work type preference)\n",
    "- **Model**: Medium (needs good conversation skills)\n",
    "- **Cost**: Similar to Tutorial 4\n",
    "- **NEW**: Asks about work type preference for filtering optimization\n",
    "\n",
    "### What's Enhanced\n",
    "\n",
    "**Tutorial 4 interview questions**:\n",
    "1. Name, skills, location, age, experience\n",
    "2. Generic follow-ups\n",
    "\n",
    "**Tutorial 5 interview questions**:\n",
    "1. Name, skills, location, age, experience\n",
    "2. **Work type preference** (remote or onsite)\n",
    "\n",
    "**Why this matters**: Getting work type preference costs us no extra tokens (same conversation length), but enables significant cost savings in the matching phase.\n",
    "\n",
    "### The Work Type Question Strategy\n",
    "\n",
    "Instead of asking vague questions about work preferences, we guide personas toward our two work types:\n",
    "- \"Do you prefer to work **remotely** from home, or do you prefer **onsite** work where you collaborate in person at an office or workplace?\"\n",
    "\n",
    "This structured approach ensures we get usable work type data that our pre-filtering can leverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_to_chat(\n",
    "    message: str,\n",
    "    persona_id: str,\n",
    "    conversation_id: str = None\n",
    ") -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"Send message to persona API and get response\"\"\"\n",
    "    url = \"https://cygeoykm2i.execute-api.us-east-1.amazonaws.com/main/chat\"\n",
    "\n",
    "    session = boto3.Session(region_name='us-east-1')\n",
    "    credentials = session.get_credentials()\n",
    "\n",
    "    payload = {\n",
    "        \"persona_id\": persona_id,\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"message\": message\n",
    "    }\n",
    "\n",
    "    request = AWSRequest(\n",
    "        method='POST',\n",
    "        url=url,\n",
    "        data=json.dumps(payload),\n",
    "        headers={'Content-Type': 'application/json'}\n",
    "    )\n",
    "    SigV4Auth(credentials, 'execute-api', 'us-east-1').add_auth(request)\n",
    "\n",
    "    response = requests.request(\n",
    "        method=request.method,\n",
    "        url=request.url,\n",
    "        headers=dict(request.headers),\n",
    "        data=request.body\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    response_json = response.json()\n",
    "    return response_json['response'], response_json['conversation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Interview Agent implementation\n",
    "INTERVIEW_PROMPT = \"\"\"\n",
    "You are conducting a career counseling interview. Gather these 6 key pieces of information:\n",
    "- Name\n",
    "- Skills and proficiency levels\n",
    "- Current location\n",
    "- Age\n",
    "- Years of experience\n",
    "- Work type: \"remote\" or \"onsite\"\n",
    "\n",
    "For job type, guide them by asking what are their preferences when it comes to work location. DO they prefer to work from home (remote) or face-to-face (onsite)\n",
    "\n",
    "Ask targeted questions to get specific information quickly.\n",
    "Do not provide job recommendations yet.\n",
    "\"\"\"\n",
    "\n",
    "def conduct_persona_interview(\n",
    "    persona_id: str,\n",
    "    max_turns: int = 5,\n",
    "    model: str = \"mistral-medium-latest\",\n",
    "    print_conversation: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"Interview a persona and return conversation transcript\"\"\"\n",
    "    \n",
    "    conversation = []\n",
    "    interview_agent = get_agent(INTERVIEW_PROMPT, model_id=model)\n",
    "    conversation_id = None\n",
    "\n",
    "    # Start with greeting\n",
    "    agent_message = \"Hello! I'm here to help you find the best opportunities. Can you tell me your name?\"\n",
    "    conversation.append(f\"Assistant: {agent_message}\")\n",
    "\n",
    "    # Conduct interview\n",
    "    for turn in range(max_turns):\n",
    "        resp = send_message_to_chat(agent_message, persona_id, conversation_id)\n",
    "        \n",
    "        if resp is None:\n",
    "            break\n",
    "            \n",
    "        user_response, conversation_id = resp\n",
    "        conversation.append(f\"User: {user_response}\")\n",
    "        \n",
    "        # Generate next question\n",
    "        agent_response = interview_agent(user_response)\n",
    "        \n",
    "        # Track cost (using utils.py function)\n",
    "        track_api_call(agent_response, model)\n",
    "        \n",
    "        agent_message = str(agent_response)\n",
    "        conversation.append(f\"Assistant: {agent_message}\")\n",
    "    \n",
    "    if print_conversation:\n",
    "        print('\\n'.join(conversation))\n",
    "        \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2: Enhanced Extraction Agents\n",
    "\n",
    "- **Purpose**: Convert unstructured text to structured data (now with work types!)\n",
    "- **Model**: Small (simple structured extraction)\n",
    "- **Cost**: Similar to Tutorial 4\n",
    "- **NEW**: Extract work type information for pre-filtering\n",
    "\n",
    "### Work Type Classification Strategy\n",
    "\n",
    "Our Job Extraction Agent uses these guidelines:\n",
    "- **Remote**: Work from home, distributed teams, virtual collaboration, location-independent\n",
    "- **Onsite**: Office-based, in-person collaboration, physical presence required, specific location needed\n",
    "\n",
    "The LLM is surprisingly good at this classification - much better than keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three enhanced extraction prompts - now with category classification\n",
    "PERSONA_EXTRACTION_PROMPT = \"\"\"Extract the following information from this conversation:\n",
    "- Name\n",
    "- Skills (as pairs of skill name and proficiency level)\n",
    "- Location\n",
    "- Age\n",
    "- Years of experience\n",
    "- Work type (one of: ['onsite', 'remote'])\n",
    "\n",
    "If work type isn't explicitly mentioned, infer from their skills and interests.\n",
    "\n",
    "Conversation:\n",
    "\"\"\"\n",
    "\n",
    "JOB_EXTRACTION_PROMPT = \"\"\"Extract from this job description:\n",
    "- Required skills (list)\n",
    "- Location\n",
    "- Years of experience required\n",
    "- Category (one of: ['physical', 'office', 'creative', 'technical', 'other'])\n",
    "\n",
    "Classify the job category based on the primary work type:\n",
    "- physical: Construction, manufacturing, agriculture, logistics, manual labor\n",
    "- office: Administration, finance, consulting, management, analysis, business\n",
    "- creative: Marketing, design, content, media, arts, communications\n",
    "- technical: Software, engineering, data science, IT, research, development\n",
    "- other: Mixed roles or unclear category\n",
    "\n",
    "Job description:\n",
    "\"\"\"\n",
    "\n",
    "TRAINING_EXTRACTION_PROMPT = \"\"\"Extract from this training description:\n",
    "- Skill taught and its level\n",
    "\n",
    "Training description:\n",
    "\"\"\"\n",
    "\n",
    "def extract_persona_info(\n",
    "    conversation: List[str],\n",
    "    model: str = \"mistral-small-latest\"\n",
    ") -> PersonaInfo:\n",
    "    \"\"\"Extract persona info from conversation using Persona Extraction Agent\"\"\"\n",
    "    text = '\\n'.join(conversation)\n",
    "    prompt = PERSONA_EXTRACTION_PROMPT + text\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model)\n",
    "    result = extraction_agent.structured_output(output_model=PersonaInfo, prompt=prompt)\n",
    "\n",
    "    # Track cost\n",
    "    if hasattr(extraction_agent, 'last_response'):\n",
    "        track_api_call(extraction_agent.last_response, model)\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_job_info(\n",
    "    path: Path,\n",
    "    model: str = \"mistral-small-latest\"\n",
    ") -> JobInfo:\n",
    "    \"\"\"Extract job info from file using Job Extraction Agent\"\"\"\n",
    "    text = load_file_content(path)\n",
    "    prompt = JOB_EXTRACTION_PROMPT + text\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model)\n",
    "    result = extraction_agent.structured_output(output_model=JobInfo, prompt=prompt)\n",
    "\n",
    "    if hasattr(extraction_agent, 'last_response'):\n",
    "        track_api_call(extraction_agent.last_response, model)\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_training_info(\n",
    "    path: Path,\n",
    "    model: str = \"mistral-small-latest\"\n",
    ") -> TrainingInfo:\n",
    "    \"\"\"Extract training info from file using Training Extraction Agent\"\"\"\n",
    "    text = load_file_content(path)\n",
    "    prompt = TRAINING_EXTRACTION_PROMPT + text\n",
    "\n",
    "    extraction_agent = get_agent(model_id=model)\n",
    "    result = extraction_agent.structured_output(output_model=TrainingInfo, prompt=prompt)\n",
    "\n",
    "    if hasattr(extraction_agent, 'last_response'):\n",
    "        track_api_call(extraction_agent.last_response, model)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's See the Other Extraction Agents in Action\n",
    "\n",
    "Now let's test our Job and Training extraction agents on real files. This shows you exactly what structured data we're pulling out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Job Extraction Agent\n",
    "print(\"üíº Testing Job Extraction Agent...\")\n",
    "print(\"Reading a sample job file...\\n\")\n",
    "\n",
    "# Get first job file\n",
    "job_paths = get_job_paths()\n",
    "if job_paths:\n",
    "    sample_job = extract_job_info(job_paths[0])\n",
    "    print(f\"Job ID: {job_paths[0].stem}\")\n",
    "    print(sample_job.describe())\n",
    "    print(\"\\nRaw Pydantic model:\")\n",
    "    print(sample_job.model_dump())\n",
    "else:\n",
    "    print(\"No job files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Training Extraction Agent\n",
    "print(\"üìö Testing Training Extraction Agent...\")\n",
    "print(\"Reading a sample training file...\\n\")\n",
    "\n",
    "# Get first training file\n",
    "training_paths = get_training_paths()\n",
    "if training_paths:\n",
    "    sample_training = extract_training_info(training_paths[0])\n",
    "    print(f\"Training ID: {training_paths[0].stem}\")\n",
    "    print(sample_training.describe())\n",
    "    print(\"\\nRaw Pydantic model:\")\n",
    "    print(sample_training.model_dump())\n",
    "else:\n",
    "    print(\"No training files found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ All three extraction agents tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 3: Optimized Matching Agent\n",
    "\n",
    "- **Purpose**: Semantic matching between personas and opportunities (with pre-filtering!)\n",
    "- **Model**: Medium (needs reasoning capabilities) \n",
    "- **NEW**: Receives pre-filtered job list instead of all available jobs\n",
    "\n",
    "### The Game-Changing Optimization\n",
    "\n",
    "**Tutorial 4 approach**:\n",
    "```python\n",
    "# Send ALL jobs to LLM every time\n",
    "jobs_text = \"\\n\".join([f'{job_id}: {job_info.describe()}' for job_id, job_info in all_jobs.items()])\n",
    "matches = find_job_matches(persona, jobs_text)  # Large token count per call\n",
    "```\n",
    "\n",
    "**Tutorial 5 approach**:\n",
    "```python\n",
    "# Pre-filter to relevant jobs only\n",
    "filtered_jobs = [job for job in all_jobs if matches_work_type(persona, job)]\n",
    "jobs_text = \"\\n\".join([f'{job_id}: {job_info.describe()}' for job_id, job_info in filtered_jobs.items()])\n",
    "matches = find_job_matches(persona, jobs_text)  # Reduced token count per call\n",
    "```\n",
    "\n",
    "**Result**: Cost reduction + better matches (LLM isn't confused by irrelevant options).\n",
    "\n",
    "### Why Pre-Filtering Works So Well\n",
    "\n",
    "**Example**: Remote software developer in S√£o Paulo\n",
    "\n",
    "**Without filtering**: LLM sees all jobs including:\n",
    "- Onsite construction jobs (wrong work type)\n",
    "- Remote marketing jobs (right work type, different skills)  \n",
    "- Onsite software jobs (right skills, wrong work type)\n",
    "- Remote software jobs (perfect matches!)\n",
    "\n",
    "**With filtering**: LLM sees only remote jobs:\n",
    "- Much easier to rank and choose the best matches\n",
    "- No confusion from work type mismatches\n",
    "\n",
    "### The Smart Filter Logic\n",
    "\n",
    "We filter jobs using work type criteria:\n",
    "1. **Work type match**: Exact work type match (remote, onsite)\n",
    "2. **Flexible handling**: Handle 'unspecified' cases appropriately\n",
    "\n",
    "This typically reduces the job pool significantly per persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_job_matches(\n",
    "    persona_info: PersonaInfo,\n",
    "    jobs_text: str,  # Pre-built context to avoid rebuilding\n",
    "    model: str = \"mistral-medium-latest\"\n",
    ") -> List[str]:\n",
    "    \"\"\"Find suitable jobs for a persona using semantic matching\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Available jobs:\n",
    "{jobs_text}\n",
    "\n",
    "Candidate profile:\n",
    "{persona_info.describe()}\n",
    "\n",
    "Return a list of up to 4 job IDs that best match this candidate.\n",
    "Consider skill transferability and semantic similarities.\n",
    "Return as a JSON list like [\"job_001\", \"job_002\"]\n",
    "\"\"\"\n",
    "\n",
    "    agent = get_agent(model_id=model)\n",
    "    response = agent(prompt)\n",
    "\n",
    "    # Track cost\n",
    "    track_api_call(response, model)\n",
    "\n",
    "    # Parse response - handle markdown code blocks\n",
    "    try:\n",
    "        response_str = str(response)\n",
    "        # Remove markdown code block markers if present\n",
    "        if response_str.startswith('```'):\n",
    "            # Extract content between code blocks\n",
    "            lines = response_str.split('\\n')\n",
    "            # Find start and end of code block\n",
    "            start_idx = 0\n",
    "            end_idx = len(lines)\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith('```') and start_idx == 0:\n",
    "                    start_idx = i + 1\n",
    "                elif line.startswith('```') and i > start_idx:\n",
    "                    end_idx = i\n",
    "                    break\n",
    "            response_str = '\\n'.join(lines[start_idx:end_idx])\n",
    "\n",
    "        result = json.loads(response_str)\n",
    "        return result if isinstance(result, list) else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def find_training_matches(\n",
    "    persona_info: PersonaInfo,\n",
    "    trainings_data: Dict[str, TrainingInfo],\n",
    "    model: str = \"mistral-medium-latest\"\n",
    ") -> List[str]:\n",
    "    \"\"\"Find suitable trainings for a persona\"\"\"\n",
    "\n",
    "    trainings_text = \"\\n\".join([\n",
    "        f'{training_id}: {training_info.describe()}'\n",
    "        for training_id, training_info in trainings_data.items()\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Available trainings:\n",
    "{trainings_text}\n",
    "\n",
    "Candidate profile:\n",
    "{persona_info.describe()}\n",
    "\n",
    "Return up to 4 training IDs that would benefit this candidate.\n",
    "Return as a JSON list like [\"tr_001\", \"tr_002\"]\n",
    "\"\"\"\n",
    "\n",
    "    agent = get_agent(model_id=model)\n",
    "    response = agent(prompt)\n",
    "\n",
    "    track_api_call(response, model)\n",
    "\n",
    "    # Parse response - handle markdown code blocks\n",
    "    try:\n",
    "        response_str = str(response)\n",
    "        # Remove markdown code block markers if present\n",
    "        if response_str.startswith('```'):\n",
    "            # Extract content between code blocks\n",
    "            lines = response_str.split('\\n')\n",
    "            # Find start and end of code block\n",
    "            start_idx = 0\n",
    "            end_idx = len(lines)\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith('```') and start_idx == 0:\n",
    "                    start_idx = i + 1\n",
    "                elif line.startswith('```') and i > start_idx:\n",
    "                    end_idx = i\n",
    "                    break\n",
    "            response_str = '\\n'.join(lines[start_idx:end_idx])\n",
    "\n",
    "        result = json.loads(response_str)\n",
    "        return result if isinstance(result, list) else []\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline (Now with Cost Optimization)\n",
    "\n",
    "Same robust pipeline as Tutorial 4, but now we track the cost savings from our optimizations.\n",
    "\n",
    "### Cost Breakdown Comparison\n",
    "\n",
    "**Tutorial 4 costs**:\n",
    "- Job extraction: Similar cost per job\n",
    "- Training extraction: Similar cost per training  \n",
    "- Persona interviews: Similar cost per persona\n",
    "- **Job matching: Higher token usage** ‚Üê The expensive part!\n",
    "\n",
    "**Tutorial 5 costs**:\n",
    "- Job extraction: Similar cost per job\n",
    "- Training extraction: Similar cost per training\n",
    "- Persona interviews: Similar cost per persona\n",
    "- **Job matching: Reduced token usage** ‚Üê Significant improvement!\n",
    "\n",
    "**Savings**: Reduced costs per run while improving accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract(\n",
    "    paths: List[Path],\n",
    "    extract_func,\n",
    "    save_path: Path,\n",
    "    cache_period: int = 20,\n",
    "    show_cost_every: int = 20\n",
    "):\n",
    "    \"\"\"Batch extract information with caching and cost tracking\n",
    "\n",
    "    Args:\n",
    "        paths: List of files to process\n",
    "        extract_func: Function to extract info from each file\n",
    "        save_path: Path to save extracted data\n",
    "        cache_period: Save progress every N items\n",
    "        show_cost_every: Display cost summary every N items\n",
    "    \"\"\"\n",
    "\n",
    "    if not save_path.exists():\n",
    "        save_json(save_path, {})\n",
    "\n",
    "    extracted = read_json(save_path)\n",
    "\n",
    "    print(f\"Processing {len(paths)} files ({len(extracted)} already cached)\")\n",
    "\n",
    "    # Reset cost tracker for this batch operation\n",
    "    if len(extracted) == 0:  # Only reset if starting fresh\n",
    "        reset_cost_tracker()\n",
    "\n",
    "    counter = 0\n",
    "    new_items_processed = 0\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        id_ = path.stem\n",
    "        if id_ not in extracted:\n",
    "            try:\n",
    "                info = extract_func(path)\n",
    "                extracted[id_] = info.model_dump_json()\n",
    "                counter += 1\n",
    "                new_items_processed += 1\n",
    "\n",
    "                # Save progress periodically\n",
    "                if counter % cache_period == 0:\n",
    "                    save_json(save_path, extracted)\n",
    "\n",
    "                # Show cost update periodically\n",
    "                if new_items_processed > 0 and new_items_processed % show_cost_every == 0:\n",
    "                    print(f\"\\nüí∞ Cost update after {new_items_processed} new items:\")\n",
    "                    print_cost_summary()\n",
    "                    print()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {id_}: {e}\")\n",
    "\n",
    "    save_json(save_path, extracted)\n",
    "\n",
    "    # Final cost summary if we processed any new items\n",
    "    if new_items_processed > 0:\n",
    "        print(f\"\\n‚úÖ Processed {new_items_processed} new items\")\n",
    "        print_cost_summary()\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all jobs\n",
    "print(\"üìÇ Processing Jobs...\")\n",
    "jobs_save_path = SUBMISSION_DIR / 'extracted_jobs.json'\n",
    "\n",
    "jobs_data = batch_extract(\n",
    "    get_job_paths(),\n",
    "    extract_job_info,\n",
    "    jobs_save_path,\n",
    "    show_cost_every=50  # Show cost update every 50 items\n",
    ")\n",
    "\n",
    "# Convert to JobInfo objects\n",
    "jobs_info = {\n",
    "    job_id: JobInfo.model_validate_json(data)\n",
    "    for job_id, data in jobs_data.items()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(jobs_info)} jobs\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all trainings\n",
    "print(\"üìÇ Processing Trainings...\")\n",
    "trainings_save_path = SUBMISSION_DIR / 'extracted_trainings.json'\n",
    "\n",
    "# Note: batch_extract now includes cost tracking!\n",
    "trainings_data = batch_extract(\n",
    "    get_training_paths(),\n",
    "    extract_training_info,\n",
    "    trainings_save_path,\n",
    "    show_cost_every=100  # Show cost update every 100 items\n",
    ")\n",
    "\n",
    "# Convert to TrainingInfo objects\n",
    "trainings_info = {\n",
    "    training_id: TrainingInfo.model_validate_json(data)\n",
    "    for training_id, data in trainings_data.items()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(trainings_info)} trainings\")\n",
    "\n",
    "# Show cumulative cost for both job and training extraction\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä Total extraction cost so far:\")\n",
    "print_cost_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Optimized System\n",
    "\n",
    "Let's test our cost optimizations with a single persona first. Watch how pre-filtering reduces the job pool!\n",
    "\n",
    "**What to observe**:\n",
    "1. The test persona has a category preference\n",
    "2. We filter jobs by **work types** before LLM matching\n",
    "3. Much fewer jobs sent to the matching agent\n",
    "4. Lower cost per matching operation\n",
    "\n",
    "**Expected improvements**:\n",
    "- Accuracy: Better focused recommendations\n",
    "- Speed: Faster due to smaller context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test persona\n",
    "test_persona = PersonaInfo(\n",
    "    name='Maria Silva',\n",
    "    skills=[('sustainability', 'intermediate'), ('project_management', 'beginner')],\n",
    "    location='S√£o Paulo',\n",
    "    age='24',\n",
    "    years_of_experience='2',\n",
    "    work_type=\"remote\"\n",
    ")\n",
    "\n",
    "print(\"üß™ Test Persona:\")\n",
    "print(test_persona.describe())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Pre-build jobs text for efficiency\n",
    "jobs_text = \"\\n\".join([\n",
    "    f'{job_id}: {job_info.describe()}'\n",
    "    for job_id, job_info in jobs_info.items()\n",
    "])\n",
    "\n",
    "# Find matches\n",
    "test_jobs = find_job_matches(test_persona, jobs_text)\n",
    "print(f\"\\nüéØ Job Matches: {test_jobs}\")\n",
    "\n",
    "test_trainings = find_training_matches(test_persona, trainings_info)\n",
    "print(f\"üìö Training Matches: {test_trainings}\")\n",
    "\n",
    "# Check cost so far\n",
    "print_cost_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interview all personas\n",
    "persona_ids = [f'persona_{i:03}' for i in range(1, 101)]\n",
    "personas_save_path = SUBMISSION_DIR / 'personas.json'\n",
    "\n",
    "if not personas_save_path.exists():\n",
    "    save_json(personas_save_path, {})\n",
    "\n",
    "persona_infos = read_json(personas_save_path)\n",
    "personas_to_process = len(persona_ids) - len(persona_infos)\n",
    "print(f'Personas to process: {personas_to_process}')\n",
    "\n",
    "# Reset cost tracker if starting fresh\n",
    "if len(persona_infos) == 0:\n",
    "    reset_cost_tracker()\n",
    "    print(\"üí∞ Starting fresh - cost tracker reset\")\n",
    "\n",
    "# Track how many new personas we process\n",
    "new_personas_processed = 0\n",
    "\n",
    "for persona_id in tqdm(persona_ids):\n",
    "    if persona_id not in persona_infos:\n",
    "        # Interview\n",
    "        conversation = conduct_persona_interview(persona_id, max_turns=3)\n",
    "\n",
    "        # Extract\n",
    "        persona_info = extract_persona_info(conversation)\n",
    "        persona_infos[persona_id] = persona_info.model_dump_json()\n",
    "        new_personas_processed += 1\n",
    "\n",
    "        # Save every 5 personas\n",
    "        if len(persona_infos) % 5 == 0:\n",
    "            save_json(personas_save_path, persona_infos)\n",
    "\n",
    "        # Show cost update every 20 personas\n",
    "        if new_personas_processed > 0 and new_personas_processed % 20 == 0:\n",
    "            print(f\"\\nüí∞ Cost update after {new_personas_processed} new personas:\")\n",
    "            print_cost_summary()\n",
    "            print()\n",
    "\n",
    "save_json(personas_save_path, persona_infos)\n",
    "\n",
    "# Convert to PersonaInfo objects\n",
    "personas = {\n",
    "    pid: PersonaInfo.model_validate_json(data)\n",
    "    for pid, data in persona_infos.items()\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Interviewed {len(personas)} personas total ({new_personas_processed} new)\")\n",
    "\n",
    "# Final cost summary for persona processing\n",
    "if new_personas_processed > 0:\n",
    "    print(\"\\nüìä Persona processing costs:\")\n",
    "    print_cost_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match trainings to jobs first\n",
    "# Pre-build jobs text ONCE for efficiency\n",
    "jobs_text = \"\\n\".join([\n",
    "    f'{job_id}: {job_info.describe()}'\n",
    "    for job_id, job_info in jobs_info.items()\n",
    "])\n",
    "\n",
    "job_training_map = {}\n",
    "for job_id, job_info in tqdm(jobs_info.items(), desc=\"Mapping trainings to jobs\"):\n",
    "    # Simple heuristic: find trainings that teach required skills\n",
    "    relevant_trainings = []\n",
    "    for tid, tinfo in trainings_info.items():\n",
    "        skill_name = tinfo.skill_acquired_and_level[0].lower()\n",
    "        if any(skill_name in req.lower() for req in job_info.required_skills):\n",
    "            relevant_trainings.append(tid)\n",
    "            if len(relevant_trainings) >= 3:\n",
    "                break\n",
    "    job_training_map[job_id] = relevant_trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset cost tracker for matching phase\n",
    "print(\"\\nüí∞ Starting matching phase - resetting cost tracker\")\n",
    "reset_cost_tracker()\n",
    "\n",
    "# Generate final results\n",
    "results = []\n",
    "personas_matched = 0\n",
    "\n",
    "for persona_id, persona_info in tqdm(personas.items(), desc=\"Generating recommendations\"):\n",
    "    data = {'persona_id': persona_id}\n",
    "\n",
    "    # CRITICAL: Check age first for awareness cases!\n",
    "    try:\n",
    "        age = int(persona_info.age) if persona_info.age and persona_info.age != 'unknown' else 25\n",
    "    except:\n",
    "        age = 25  # Default to adult if age parsing fails\n",
    "\n",
    "    if age < 16:\n",
    "        # Minor - needs awareness type\n",
    "        data['predicted_type'] = 'awareness'\n",
    "        data['predicted_items'] = 'too_young'\n",
    "    else:\n",
    "        # Adult - proceed with job/training matching\n",
    "        filtered_jobs = {}\n",
    "        for job_id, job_info in jobs_info.items():\n",
    "            if persona_info.work_type == 'unspecified':\n",
    "                filtered_jobs[job_id] = job_info\n",
    "            elif job_info.work_type == 'unspecified':\n",
    "                filtered_jobs[job_id] = job_info\n",
    "            elif persona_info.work_type == job_info.work_type:\n",
    "                filtered_jobs[job_id] = job_info\n",
    "\n",
    "        # Build context with filtered jobs only (major token savings!)\n",
    "        jobs_text = \"\\n\".join([\n",
    "            f'{job_id}: {job_info.describe()}'\n",
    "            for job_id, job_info in filtered_jobs.items()\n",
    "        ])\n",
    "\n",
    "        jobs = find_job_matches(persona_info, jobs_text)\n",
    "        personas_matched += 1\n",
    "\n",
    "        if jobs:\n",
    "            data['predicted_type'] = 'jobs+trainings'\n",
    "            data['jobs'] = [\n",
    "                {\n",
    "                    'job_id': job_id,\n",
    "                    'suggested_trainings': job_training_map.get(job_id, [])\n",
    "                }\n",
    "                for job_id in jobs\n",
    "            ]\n",
    "        else:\n",
    "            # No jobs found, suggest trainings only\n",
    "            trainings = find_training_matches(persona_info, trainings_info)\n",
    "            data['predicted_type'] = 'trainings_only'\n",
    "            data['trainings'] = trainings\n",
    "\n",
    "    results.append(data)\n",
    "\n",
    "    # Show cost update every 25 personas\n",
    "    if personas_matched % 25 == 0 and personas_matched > 0:\n",
    "        print(f\"\\nüí∞ Matching progress - {personas_matched} personas matched:\")\n",
    "        print_cost_summary()\n",
    "        print()\n",
    "\n",
    "# Save results\n",
    "results_save_path = SUBMISSION_DIR / 'results.json'\n",
    "save_json(results_save_path, results)\n",
    "print(f\"\\n‚úÖ Generated recommendations for {len(results)} personas\")\n",
    "print(f\"üìÅ Results saved to: {results_save_path}\")\n",
    "\n",
    "# Count types for debugging\n",
    "type_counts = {}\n",
    "for r in results:\n",
    "    t = r.get('predicted_type', 'unknown')\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "print(f\"\\nüìä Type distribution: {type_counts}\")\n",
    "\n",
    "# Final cost summary for matching\n",
    "print(\"\\nüìä Final matching costs:\")\n",
    "print_cost_summary()\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Leaderboard!\n",
    "\n",
    "This is it. The moment of truth. If everything worked, you should see your score improving!\n",
    "\n",
    "**Before submitting:**\n",
    "- Check you have 100 results (one per persona)\n",
    "- Make sure you're not submitting your 10th attempt today (there's a limit!)\n",
    "\n",
    "**After submitting:**\n",
    "- Go check the leaderboard immediately\n",
    "- Screenshot your score for bragging rights\n",
    "- Share what worked in the Teams channel (help others, win the collaborator award!)\n",
    "- If your score is still terrible, check our debugging tips above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import make_submission\n",
    "\n",
    "# Submit\n",
    "response = make_submission(results)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    print(\"üéâ Submission successful! Check the leaderboard!\")\n",
    "else:\n",
    "    print(f\"‚ùå Submission failed: {response.text if response else 'No response'}\")\n",
    "\n",
    "# Final cost report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print_cost_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Just Built (Tutorial 5 Achievements!)\n",
    "\n",
    "Congrats! You went from Tutorial 4's baseline to Tutorial 5's optimized system. **This is production-ready code.**\n",
    "\n",
    "‚úÖ **Work type-based pre-filtering** - Significant token reduction while improving accuracy\n",
    "\n",
    "‚úÖ **Production cost optimization** - Reduced costs per run with better performance\n",
    "\n",
    "‚úÖ **Enhanced data models** with work type classification \n",
    "\n",
    "‚úÖ **Smart interview questions** that gather filtering info efficiently\n",
    "\n",
    "‚úÖ **Pre-filtering pipeline** that reduces noise before LLM matching\n",
    "\n",
    "‚úÖ **Improved accuracy** with lower costs (the holy grail of optimization!)\n",
    "\n",
    "### Tutorial 5 Score Analysis\n",
    "\n",
    "- **If you got high scores**: Perfect! You've mastered production-level optimization\n",
    "- **If you got moderate scores**: Good progress. Check if your work type filtering is working\n",
    "- **If you got similar to Tutorial 4**: Your filtering might not be aggressive enough. Debug the pre-filter step\n",
    "- **If you got lower scores**: Something's wrong with the work type extraction or filtering logic\n",
    "\n",
    "### Key Tutorial 5 Learnings\n",
    "\n",
    "1. **Pre-filtering beats post-processing** - Filter data before expensive LLM calls\n",
    "2. **Work types are powerful** - Simple classification enables major optimizations  \n",
    "3. **Token optimization = cost optimization** - Every token saved is money saved\n",
    "4. **Better data > better models** - Clean, filtered data beats raw compute power\n",
    "5. **Production thinking** - Optimize for cost AND accuracy, not just accuracy\n",
    "\n",
    "## Tutorial 4 vs Tutorial 5 Comparison\n",
    "\n",
    "| Metric | Tutorial 4 | Tutorial 5 | Improvement |\n",
    "|--------|------------|------------|-------------|\n",
    "| **Accuracy** | Baseline | Improved | Better |\n",
    "| **Cost per run** | Higher | Lower | Reduced |\n",
    "| **Tokens per match** | More | Fewer | Optimized |\n",
    "| **Jobs considered per persona** | All available | Filtered subset | Focused |\n",
    "| **Production readiness** | Demo | Ready | ‚úÖ |\n",
    "\n",
    "## Real-World Impact\n",
    "\n",
    "**What you built**: A system that can handle large volumes efficiently with work type optimization.\n",
    "\n",
    "**What consulting clients see**: \"Better accuracy, lower cost, faster results.\"\n",
    "\n",
    "**What you learned**: Production optimization is about smart data flow, not just smart algorithms.\n",
    "\n",
    "## Next Steps (Advanced Optimizations)\n",
    "\n",
    "You've mastered the fundamentals. Here's what advanced teams might be doing:\n",
    "- **Caching similar personas** (if personas have similar profiles, reuse recommendations)\n",
    "- **Batch processing** (process multiple personas in single LLM calls)\n",
    "- **Dynamic filtering** (adjust filter strictness based on available jobs)\n",
    "- **Multi-stage matching** (coarse filter ‚Üí fine filter ‚Üí final ranking)\n",
    "\n",
    "**Remember**: You're not just building a hackathon project. You're building skills that matter in production AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Tutorial 5 Exercises (Advanced Optimizations)\n",
    "\n",
    "### Exercise 1: Dynamic Filtering Thresholds\n",
    "What if no jobs match the exact work type? Implement fallback logic:\n",
    "- First try: exact work type match  \n",
    "- Fallback 1: include 'unspecified' work types\n",
    "- Fallback 2: expand to broader matching criteria\n",
    "\n",
    "### Exercise 2: Batch Interview Processing  \n",
    "Instead of interviewing personas one-by-one, can you interview multiple at once? Design a group interview prompt and measure improvements.\n",
    "\n",
    "### Exercise 3: Smart Caching by Similarity\n",
    "If two personas have similar profiles (same location, work type, skill level), can you reuse job matches? Build a similarity detector and cache system.\n",
    "\n",
    "### Exercise 4: Multi-Stage Filtering\n",
    "Implement a two-stage filter:\n",
    "- Stage 1: Basic filter (location + work type) \n",
    "- Stage 2: Skills-based filter (remove jobs requiring skills they don't have)\n",
    "- Result: Even more focused job lists for LLM\n",
    "\n",
    "### Exercise 5: Work Type Learning\n",
    "Track which work type classifications work best. Can you improve the extraction prompts based on success patterns?\n",
    "\n",
    "**Advanced Challenge**: Combine ALL optimizations and aim for even higher accuracy at lower cost per run.\n",
    "\n",
    "Share your breakthroughs in Teams! The most innovative optimization gets a special shoutout. üèÜ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
