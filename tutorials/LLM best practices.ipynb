{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10e881c",
   "metadata": {},
   "source": [
    "# üß†üìâ Onboarding Notebook ‚Äî Eco- & Cost-Efficient Use of Mistral LLM via *La Plateforme* \n",
    "> Version generated on 2025-09-09\n",
    "\n",
    "\n",
    "Welcome! This notebook is part of the training material for your **data science challenge**.\n",
    "\n",
    "**Context:** You will build a solution that interacts with **100 personas** (simulated by LLMs) in natural language to guide their professional career. Each persona has an associated **gold file** describing the ground-truth recommendations your solution should steer them toward.\n",
    "\n",
    "**Goal:** Help you use Mistral LLM efficiently ‚Äî minimizing **cost**, **latency**, and **environmental impact** while keeping outputs reliable and reproducible for **testing vs. final submission**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424c0d2",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Prerequisites\n",
    "\n",
    "- **Account** and **API key** for Mistral (*La Plateforme*). Store it in the environment variable `MISTRAL_API_KEY`.\n",
    "- Python ‚â• 3.9, and (optionally) these packages:\n",
    "```bash\n",
    "pip install mistralai python-dotenv pandas matplotlib \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680a1d0",
   "metadata": {},
   "source": [
    "## 1) Setup: API key, client, quick verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"‚ö†Ô∏è Please set MISTRAL_API_KEY in your environment or a .env file.\")\n",
    "\n",
    "try:\n",
    "    from mistralai import Mistral\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install the SDK first: pip install mistralai\") from e\n",
    "\n",
    "client = Mistral(api_key=API_KEY)\n",
    "\n",
    "# Optional: list available models (fallback to a known one if unavailable)\n",
    "try:\n",
    "    available_models = [m.id for m in client.models.list().data]  # type: ignore\n",
    "    print(\"‚úÖ Available models:\", available_models)\n",
    "except Exception:\n",
    "    print(\"‚ÑπÔ∏è Could not list models; proceed with a known one (e.g., 'mistral-small-latest').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699a61b",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Efficiency Principles (cost, latency, energy)\n",
    "\n",
    "1. **Start small**: prototype with a *small* model first; scale only if needed.  \n",
    "2. **Strict bounds**: set conservative `max_tokens` and increase only with evidence.  \n",
    "3. **Concise prompts**: bullet points, explicit schemas, JSON output.  \n",
    "4. **Stop sequences**: stop generation when your structure is complete.  \n",
    "5. **Caching**: cache stable prompts and documents; hash inputs.  \n",
    "6. **Measure**: track tokens/latency/errors; **enforce budgets**.  \n",
    "7. **Determinism**: low temperature; seed if supported; avoid redundant retries.  \n",
    "8. **Clean inputs**: remove noise and unnecessary history.\n",
    "9. **Carbon aware scheduling**:Schedule heavy runs during low-carbon hours (e.g., nighttime or weekends in your region) if using cloud infrastructure. This can reduce environmental impact when used at scale.\n",
    "10. **Submission fingerprinting**:Include model version, temperature, token usage, and prompt hash. This helps with reproducibility and auditability without storing full logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719dd32",
   "metadata": {},
   "source": [
    "## 2) Minimal call with strict token control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This snippet shows how to make a safe, efficient, and reproducible call to the Mistral LLM using a structured prompt and controlled generation settings.\n",
    "\n",
    "What the Code Does:\n",
    "Defines a system prompt that sets clear behavior: concise answers under 60 words, and honesty when uncertain.\n",
    "\n",
    "Sends a user query asking why limiting max_tokens is important.\n",
    "\n",
    "Calls the Mistral model with:\n",
    "\n",
    "max_tokens=120: limits the length of the response to control cost and latency.\n",
    "temperature=0.2: encourages deterministic, focused output.\n",
    "top_p=0.9: controls diversity of output (optional).\n",
    "(Optional) stop and response_format parameters for cleaner output or structured JSON.\n",
    "Prints the model‚Äôs reply and usage stats:\n",
    "\n",
    "prompt_tokens: tokens used in the input\n",
    "completion_tokens: tokens used in the output\n",
    "total_tokens: total cost-relevant token count\n",
    "\"\"\"\n",
    "\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "MODEL_SMALL = \"mistral-small-latest\"  # adjust to your org's models\n",
    "\n",
    "system = \"You are a concise assistant. Answer in <60 words. If unsure, say so.\"\n",
    "user = \"Explain why limiting max_tokens is important when calling an LLM.\"\n",
    "\n",
    "resp = client.chat.complete(\n",
    "    model=MODEL_SMALL,\n",
    "    messages=[\n",
    "        ChatMessage(role=\"system\", content=system),\n",
    "        ChatMessage(role=\"user\", content=user),\n",
    "    ],\n",
    "    max_tokens=120,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    # stop=[\"\\nEND\"]  # add if you know your stopping delimiter\n",
    "    # response_format={\"type\":\"json_object\"}  # add if you want JSON\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)   # type: ignore\n",
    "print(\"usage:\", resp.usage)  # prompt_tokens, completion_tokens, total_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9f0cf",
   "metadata": {},
   "source": [
    "## 3) Compact JSON output (less verbosity, easier post-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ac95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This code demonstrates how to request structured output from the Mistral LLM using the response_format={\"type\": \"json_object\"} parameter.\n",
    "\n",
    "What the Code Does:\n",
    "Defines a prompt asking the model to give 3 reasons to reduce token usage, and to reply in a specific JSON format.\n",
    "Sends the prompt to the Mistral model (MODEL_SMALL) with:\n",
    "max_tokens=120: limits the response length.\n",
    "temperature=0.2: ensures low variability for reproducibility.\n",
    "response_format={\"type\": \"json_object\"}: instructs the model to return a valid JSON object.\n",
    "Prints the model‚Äôs response and the token usage metadata.\"\"\"\n",
    "\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "prompt = \"Give 3 reasons to reduce tokens. Reply in JSON: {reasons:[...]}\"\n",
    "\n",
    "resp_json = client.chat.complete(\n",
    "    model=MODEL_SMALL,\n",
    "    messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "    max_tokens=120,\n",
    "    temperature=0.2,\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    ")\n",
    "\n",
    "print(resp_json.choices[0].message.content)  # type: ignore\n",
    "print(\"usage:\", resp_json.usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76203d27",
   "metadata": {},
   "source": [
    "## 4) Simple local cache (avoid unnecessary model calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b62ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This utility helps you cache LLM responses locally to avoid repeated API calls during development and testing. It‚Äôs designed to reduce cost, latency, and environmental impact while maintaining consistent behavior.\n",
    "\n",
    "What the Code Does:\n",
    "Creates a cache directory (.mistral_cache) to store responses.\n",
    "Generates a unique cache key using a SHA256 hash of:\n",
    "Model name\n",
    "Messages\n",
    "Other parameters (e.g., temperature, max_tokens)\n",
    "Checks if a cached response exists:\n",
    "If yes, loads it from disk and marks it as \"from_cache\": True\n",
    "If not, calls the Mistral API, saves the response to disk, and marks it as \"from_cache\": False\n",
    "Returns the response content and usage metadata.\"\"\"\n",
    "\n",
    "import hashlib, json, os, time\n",
    "\n",
    "CACHE_DIR = \".mistral_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def _cache_key(model:str, messages:list, **kwargs)->str:\n",
    "    blob = json.dumps({\"model\":model, \"messages\":messages, **kwargs}, sort_keys=True).encode()\n",
    "    return hashlib.sha256(blob).hexdigest()\n",
    "\n",
    "def chat_with_cache(model:str, messages:list, **kwargs):\n",
    "    key = _cache_key(model, messages, **kwargs)\n",
    "    path = os.path.join(CACHE_DIR, key + \".json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data[\"from_cache\"] = True\n",
    "        return data\n",
    "    out = client.chat.complete(model=model, messages=messages, **kwargs)\n",
    "    data = {\n",
    "        \"content\": out.choices[0].message.content,  # type: ignore\n",
    "        \"usage\": getattr(out, \"usage\", None),\n",
    "        \"created\": int(time.time())\n",
    "    }\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    data[\"from_cache\"] = False\n",
    "    return data\n",
    "\n",
    "test = chat_with_cache(\n",
    "    MODEL_SMALL,\n",
    "    [ChatMessage(role=\"user\", content=\"One short sentence on why caching matters.\")],\n",
    "    max_tokens=40,\n",
    "    temperature=0.1\n",
    ")\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2af4c",
   "metadata": {},
   "source": [
    "## 5) Track usage and enforce budgets (lightweight logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e587c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This code helps you track and log the usage of a language model (LLM) like Mistral during your experiments. It‚Äôs especially useful when you're testing prompts, measuring latency, or analyzing token consumption.\n",
    "\n",
    "What Does This Code Do?\n",
    "Defines a structure to store usage data\n",
    "Logs each LLM call to a CSV file\n",
    "Wraps the LLM call in a safe function that handles errors and logs performance\n",
    "\"\"\"\n",
    "\n",
    "import csv, time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "LOG_PATH = Path(\"mistral_usage_log.csv\")\n",
    "\n",
    "@dataclass\n",
    "class UsageRecord:\n",
    "    ts: float\n",
    "    model: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_s: float\n",
    "    ok: bool\n",
    "\n",
    "def log_usage(rec: UsageRecord):\n",
    "    exists = LOG_PATH.exists()\n",
    "    with open(LOG_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow([\"ts\",\"model\",\"prompt_tokens\",\"completion_tokens\",\"total_tokens\",\"latency_s\",\"ok\"])\n",
    "        w.writerow([rec.ts, rec.model, rec.prompt_tokens, rec.completion_tokens, rec.total_tokens, rec.latency_s, rec.ok])\n",
    "\n",
    "def safe_chat(model:str, messages:list, **kwargs)->str:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        out = client.chat.complete(model=model, messages=messages, **kwargs)\n",
    "        latency = time.time() - t0\n",
    "        u = getattr(out, \"usage\", None) or {}\n",
    "        log_usage(UsageRecord(time.time(), model, u.get(\"prompt_tokens\",0), u.get(\"completion_tokens\",0), u.get(\"total_tokens\",0), latency, True))\n",
    "        return out.choices[0].message.content  # type: ignore\n",
    "    except Exception as e:\n",
    "        latency = time.time() - t0\n",
    "        log_usage(UsageRecord(time.time(), model, 0, 0, 0, latency, False))\n",
    "        raise\n",
    "\n",
    "# Example\n",
    "txt = safe_chat(\n",
    "    MODEL_SMALL,\n",
    "    [ChatMessage(role=\"user\", content=\"Reply with just 'ok'\")],\n",
    "    max_tokens=3, temperature=0\n",
    ")\n",
    "print(txt)\n",
    "print(f\"Log file: {LOG_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c9566",
   "metadata": {},
   "source": [
    "### 6)Visualize usage quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This code helps analyze your LLM usage over time by reading the mistral_usage_log.csv file and plotting how many tokens were used in each call.\n",
    "\n",
    "What the Code Does:\n",
    "Checks if the usage log file exists (mistral_usage_log.csv).\n",
    "If it exists:\n",
    "Loads the log into a pandas DataFrame.\n",
    "Displays the last 5 entries to give a quick snapshot of recent usage.\n",
    "Plots the total_tokens column to show how token usage varies across calls.\n",
    "If the file doesn‚Äôt exist:\n",
    "Prints a message saying no logs are available yet.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if LOG_PATH.exists():\n",
    "    df = pd.read_csv(LOG_PATH)\n",
    "    display(df.tail(5))\n",
    "    plt.figure()\n",
    "    df['total_tokens'].plot(title=\"Total tokens per call\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No logs yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0f78e",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Choosing the right model\n",
    "\n",
    "- **Heuristic**: *small* for extraction/formatting/JSON, *medium* for moderate reasoning, *large* only if strictly necessary.  \n",
    "- **Local benchmark**: create a small eval set (20‚Äì50 cases); compare **quality vs. cost/latency**.  \n",
    "- **Solution profiles**: define presets (`FAST_CHEAP`, `BALANCED`, `SMART`) and **default to the frugal one**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42039ad7",
   "metadata": {},
   "source": [
    "## 8) Robustness: timeouts, retries, exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56913cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This code defines a retry mechanism that wraps your Mistral API call to make it more robust and fault-tolerant. If the call fails due to a transient error (like a network glitch or rate limit), it will automatically retry up to 3 times using exponential backoff with jitter‚Äîa technique that increases the wait time between retries while adding randomness to avoid overloading the server.\"\"\"\n",
    "import random, time\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "def retry(fn, attempts=3, base=0.5, jitter=0.3):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception:\n",
    "            if i == attempts - 1:\n",
    "                raise\n",
    "            delay = base * (2**i) + random.uniform(0, jitter)\n",
    "            time.sleep(delay)\n",
    "\n",
    "res = retry(lambda: client.chat.complete(\n",
    "    model=MODEL_SMALL,\n",
    "    messages=[ChatMessage(role=\"user\", content=\"Reply 'ok'\")],\n",
    "    max_tokens=3, temperature=0\n",
    "))\n",
    "print(res.choices[0].message.content)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5018e",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Governance, Security & Privacy\n",
    "\n",
    "- **Log only essentials** (hash inputs rather than storing raw text if feasible).  \n",
    "- **Guardrails**: input size limits, content filters, output JSON schema validation.  \n",
    "- **Version prompts & configs** (git) and link results to artifact hashes.  \n",
    "- **Separate environments** (local/submission) and API keys with roles/quotas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e343345",
   "metadata": {},
   "source": [
    "\n",
    "## 10) (Optional) Rough environmental impact estimation\n",
    "\n",
    "> Exact footprint of API calls is not directly observable. You can still track tokens and apply an **internal factor** (gCO‚ÇÇe per 1k tokens) for relative comparisons.\n",
    "\n",
    "Formula: `impact ‚âà total_tokens √ó factor_gco2_per_1k`. Use for **relative** comparisons only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5789d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This script helps track and estimate the environmental impact of Mistral LLM usage by calculating the carbon footprint based on total tokens used.It reads mistral usage log that contains token counts per run.It applies a simple formula to estimate emissions. It then adds a new column est_gco2 to the log and displays the last 10 entries.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "G_CO2_PER_1K_TOKENS = 2.85   #Grams of carbon dioxide equivalent per 1000 tokens\n",
    "\n",
    "def estimate_emissions_gco2(total_tokens:int, factor_per_1k:float=G_CO2_PER_1K_TOKENS)->float:\n",
    "    return (total_tokens / 1000.0) * factor_per_1k\n",
    "\n",
    "from pathlib import Path\n",
    "LOG_PATH = Path(\"mistral_usage_log.csv\")\n",
    "\n",
    "if LOG_PATH.exists():\n",
    "    df = pd.read_csv(LOG_PATH)\n",
    "    df[\"est_gco2\"] = df[\"total_tokens\"].apply(estimate_emissions_gco2)\n",
    "    display(df[[\"ts\",\"model\",\"total_tokens\",\"est_gco2\"]].tail(10))\n",
    "    print(f\"Total estimate (gCO2e): {df['est_gco2'].sum():.2f}\")\n",
    "else:\n",
    "    print(\"No usage log yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab10de",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Frugality checklist **before final submission**\n",
    "\n",
    "- [ ] **Smallest sufficient model** is default.  \n",
    "- [ ] `max_tokens` and **stop** set.  \n",
    "- [ ] **JSON output** where relevant; **compact prompts**.  \n",
    "- [ ] **Cache** used in local testing; **disabled for submission** (no stale answers).  \n",
    "- [ ] **Budgets** in place (tokens/calls/day).  \n",
    "- [ ] **Usage logging** (tokens, latency, errors) + minimal dashboard.  \n",
    "- [ ] **Eval set** (20‚Äì50 cases) shows quality/cost/latency trade-offs.  \n",
    "- [ ] **Privacy**: minimization, guardrails, lean logs.  \n",
    "- [ ] **Retries/backoff** + **timeouts** configured.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07ec86",
   "metadata": {},
   "source": [
    "## 12) Assistant specifics: conversational memory & rolling summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b000b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Token counting & rolling summary utilities\n",
    "\"\"\"This utility helps track token usage and manage prompt length efficiently when working with Mistral LLM.\n",
    "\n",
    "What it does:\n",
    "Estimates token count using tiktoken to approximate how many tokens a message will consume.\n",
    "Trims message history to stay within a token budget, keeping only the most relevant parts (e.g., system prompt + last user turns).\n",
    "Maintains a rolling summary of the conversation using the model itself, summarizing recent turns into a compact bullet list. This helps reduce prompt size while preserving context.\"\"\"\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "from typing import List\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "ENC = tiktoken.get_encoding(\"cl100k_base\")  # approx for counting\n",
    "\n",
    "def approx_tokens(text: str) -> int:\n",
    "    return len(ENC.encode(text))\n",
    "\n",
    "def total_tokens_messages(msgs: List[ChatMessage]) -> int:\n",
    "    return sum(approx_tokens(getattr(m, \"content\", \"\")) for m in msgs)\n",
    "\n",
    "def trim_messages(msgs: List[ChatMessage], max_prompt_tokens: int) -> List[ChatMessage]:\n",
    "    system = [m for m in msgs if m.role == \"system\"]\n",
    "    others = [m for m in msgs if m.role != \"system\"]\n",
    "    base = (system[:1] + others[-2:]) if len(others) >= 2 else msgs[:]\n",
    "    if total_tokens_messages(base) <= max_prompt_tokens:\n",
    "        return base\n",
    "    return (system[:1] + [m for m in others if m.role == \"user\"][-1:])\n",
    "\n",
    "ROLLING_SUMMARY = \"\"\n",
    "\n",
    "def update_rolling_summary(client, model, msgs: List[ChatMessage]) -> str:\n",
    "    global ROLLING_SUMMARY\n",
    "    prompt = [\n",
    "        ChatMessage(role=\"system\", content=\"Summarize into ‚â§5 factual bullets, in English, no verbosity.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Current summary:\\n{ROLLING_SUMMARY}\\n\\nNew turns:\\n{[(m.role, m.content) for m in msgs[-3:]]}\\n\\nUpdate the summary.\"),\n",
    "    ]\n",
    "    out = client.chat.complete(model=model, messages=prompt, max_tokens=120, temperature=0.1)\n",
    "    ROLLING_SUMMARY = out.choices[0].message.content  # type: ignore\n",
    "    return ROLLING_SUMMARY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61939dd8",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Minimal system prompt for assistants\n",
    "\n",
    "Keep the **contract short** to save tokens each turn:\n",
    "\n",
    "> *\"You are a concise English assistant. Answer in ‚â§120 words. If a table is requested, return valid JSON `{columns:[], rows:[]}`. For code, return code only. If unsure, say so and ask one clarifying question. Use only the provided context.\"*\n",
    "Replace verbose instructions with symbolic or shorthand cues (e.g., #plan, #tool_call) that are interpreted by the model but save tokens. This is especially useful in multi-step agentic flows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b5c5c",
   "metadata": {},
   "source": [
    "## 14) Agentic AI: frugal planning, tool use, limits and budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38554738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#??? replace with gdsc skeleton solution\n",
    "# Frugal agent skeleton (toy tools + simple policy)\n",
    "import time, json\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "TOOLS = {\n",
    "    \"search_docs\": lambda q: \"synthetic results...\",\n",
    "    \"get_kpis\": lambda _: {\"revenue\": 1234, \"growth\": 0.12},\n",
    "}\n",
    "\n",
    "def select_tool_heuristic(instruction: str) -> str:\n",
    "    lo = instruction.lower()\n",
    "    if any(k in lo for k in [\"kpi\",\"revenue\",\"metric\"]):\n",
    "        return \"get_kpis\"\n",
    "    if any(k in lo for k in [\"doc\",\"documentation\",\"guide\"]):\n",
    "        return \"search_docs\"\n",
    "    return \"\"\n",
    "\n",
    "def agent_answer(query: str, max_steps: int = 4, token_budget: int = 1200):\n",
    "    steps, used_tokens = 0, 0\n",
    "    trace = []\n",
    "\n",
    "    system = \"Concise English assistant. Prefer short JSON when useful. Use at most one tool.\"\n",
    "    messages = [ChatMessage(role=\"system\", content=system),\n",
    "                ChatMessage(role=\"user\", content=query)]\n",
    "\n",
    "    while steps < max_steps and used_tokens < token_budget:\n",
    "        steps += 1\n",
    "        tool = select_tool_heuristic(query)\n",
    "        if tool:\n",
    "            t0 = time.time()\n",
    "            result = TOOLS[tool](query)\n",
    "            latency = time.time() - t0\n",
    "            trace.append({\"step\": steps, \"tool\": tool, \"latency_s\": latency, \"preview\": str(result)[:160]})\n",
    "            messages.append(ChatMessage(role=\"assistant\", content=f\"[TOOL:{tool}] {str(result)[:800]}\"))\n",
    "\n",
    "        out = client.chat.complete(model=MODEL_SMALL, messages=messages, max_tokens=160, temperature=0.2)\n",
    "        reply = out.choices[0].message.content  # type: ignore\n",
    "        u = getattr(out, \"usage\", {})\n",
    "        used_tokens += u.get(\"total_tokens\", 0)\n",
    "        messages.append(ChatMessage(role=\"assistant\", content=reply))\n",
    "\n",
    "        if reply.strip().endswith(\".\"):\n",
    "            break\n",
    "\n",
    "    return reply, trace, used_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11aebc",
   "metadata": {},
   "source": [
    "## 15) Function-calling (if supported): minimal schemas & stop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2551dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This utility defines a tool specification for reading Excel file content via a structured function call, enabling integration with chat-based AI systems.\n",
    "What it does:\n",
    "*Declares a function named get_excel_content for use in tool-based AI workflows.\n",
    "*Enables reading data from an Excel file by specifying:\n",
    "  -File path\n",
    "  -Sheet name\n",
    "  -Optional cell range (e.g., \"A1:C10\")\n",
    "*Supports automatic tool selection (tool_choice=\"auto\") for intelligent invocation.\n",
    "*Ensures controlled output via max_tokens, temperature, and stop conditions.\"\"\"\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_excel_content\",\n",
    "      \"description\": \"Reads and returns content from an Excel file. You can specify sheet name and optionally a cell range.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"file_path\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the Excel file.\"\n",
    "          },\n",
    "          \"sheet_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the sheet to read from.\"\n",
    "          },\n",
    "          \"range\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Optional cell range (e.g., 'A1:C10').\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"file_path\", \"sheet_name\"],\n",
    "        \"additionalProperties\": false\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "# Example call (adapt to your SDK version):\n",
    "# resp = client.chat.complete(\n",
    "#   model=MODEL_SMALL,\n",
    "#   messages=[...],\n",
    "#   tools=TOOLS_SPEC,\n",
    "#   tool_choice=\"auto\",\n",
    "#   max_tokens=160,\n",
    "#   temperature=0.2,\n",
    "#   stop=[\"</tool_output>\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a64f13",
   "metadata": {},
   "source": [
    "## 16) Agent observability: lean logs, metrics, and circuit breakers with submission fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This utility logs each agent run to a CSV file (agent_runs.csv) for traceability, debugging, and performance analysis.\n",
    "\n",
    "What it does:\n",
    "Records each tool invocation made during an agent run.\n",
    "Captures the following details for each step:\n",
    "Timestamp (in human-readable ISO format)\n",
    "Run ID\n",
    "Step number\n",
    "Tool name\n",
    "Latency (in seconds)\n",
    "Total tokens used for the run\n",
    "Automatically creates the CSV file with headers if it doesn't exist.\n",
    "Uses structured logging via csv.DictWriter for clarity and consistency.\n",
    "Includes error handling to ensure robustness during file operations.\"\"\"\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "AGENT_LOG = Path(\"agent_runs.csv\")\n",
    "\n",
    "def log_agent_run(run_id: str, trace: list, used_tokens: int):\n",
    "    fieldnames = [\"timestamp\", \"run_id\", \"step\", \"tool\", \"latency_s\", \"used_tokens\"]\n",
    "    log_exists = AGENT_LOG.exists()\n",
    "\n",
    "    try:\n",
    "        with open(AGENT_LOG, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if not log_exists:\n",
    "                writer.writeheader()\n",
    "            for t in trace:\n",
    "                writer.writerow({\n",
    "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                    \"run_id\": run_id,\n",
    "                    \"step\": t.get(\"step\"),\n",
    "                    \"tool\": t.get(\"tool\", \"\"),\n",
    "                    \"latency_s\": t.get(\"latency_s\", 0.0),\n",
    "                    \"used_tokens\": used_tokens\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging agent run: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"This code helps you track each agent run and generate a minimal fingerprint for your final submission. \n",
    "\n",
    "What the Code Does:\n",
    "Defines a log file (agent_runs.csv) to store metadata about each run.\n",
    "Generates a SHA256 hash of the prompt using hashlib, so you don‚Äôt need to store raw prompt text.\n",
    "Logs each run with:\n",
    "Timestamp (ts)\n",
    "Run ID (run_id)\n",
    "Model name (model)\n",
    "Temperature (temperature)\n",
    "Total tokens used (total_tokens)\n",
    "Prompt hash (prompt_hash)\n",
    "Prints a fingerprint in JSON format that you can include with your final submission.\n",
    "\"\"\"\n",
    "\n",
    "import csv, time, hashlib, json\n",
    "from pathlib import Path\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "MODEL_SMALL = \"mistral-small-latest\"\n",
    "AGENT_LOG = Path(\"agent_runs.csv\")\n",
    "\n",
    "def hash_prompt(messages):\n",
    "    full_prompt = \"\\n\".join(f\"{m.role}: {m.content}\" for m in messages)\n",
    "    return hashlib.sha256(full_prompt.encode()).hexdigest()\n",
    "\n",
    "def log_agent_run(run_id: str, messages: list, used_tokens: int, temperature: float):\n",
    "    exists = AGENT_LOG.exists()\n",
    "    prompt_hash = hash_prompt(messages)\n",
    "    timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "\n",
    "    with open(AGENT_LOG, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow([\"ts\", \"run_id\", \"model\", \"temperature\", \"total_tokens\", \"prompt_hash\"])\n",
    "        w.writerow([timestamp, run_id, MODEL_SMALL, temperature, used_tokens, prompt_hash])\n",
    "\n",
    "    # Optional: print fingerprint for submission\n",
    "    fingerprint = {\n",
    "        \"model\": MODEL_SMALL,\n",
    "        \"temperature\": temperature,\n",
    "        \"total_tokens_used\": used_tokens,\n",
    "        \"prompt_hash\": prompt_hash\n",
    "    }\n",
    "    print(\"Submission Fingerprint:\")\n",
    "    print(json.dumps(fingerprint, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e058c46",
   "metadata": {},
   "source": [
    "\n",
    "## 17) Challenge specifics: **testing vs submission** with personas & gold files\n",
    "\n",
    "- **Personas**: 100 LLM-driven personas to interact with in natural language.  \n",
    "- **Gold files**: each persona has a ground-truth recommendation set your solution should surface.  \n",
    "- **Testing** (local): smaller models, aggressive caching, verbose logs, sandboxed tools.  \n",
    "- **Submission** (final run): deterministic settings (low temperature), **no cache reuse**, strict budgets, concise prompts, validated JSON outputs, lean logs.  \n",
    "- **Evaluation tip**: build a small **offline eval set** mapping persona snippets ‚Üí expected gold recommendations, measure accuracy + tokens/latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c39e28a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "üéØ **Key takeaway**: *Start small, measure everything, enforce limits, and design for frugality from the start.*  \n",
    "Good luck with the challenge!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
